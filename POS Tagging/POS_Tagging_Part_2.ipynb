{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODsy8a1NCUdjU+pLzWwG12"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRXODnxy0Thc"
      },
      "source": [
        "# **POS Tagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K8IUY23KfIf"
      },
      "source": [
        "# PART 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqHnUDfH1eh5"
      },
      "source": [
        "## Installs, Imports and Paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZrIDPRbGfjOq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_learn\n",
        "!pip install nltk\n",
        "!pip install --upgrade wandb\n",
        "!pip install datasets\n",
        "!pip install tensorflow\n",
        "!pip install scipy\n",
        "!pip install scikeras\n",
        "!pip install numpy\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awcSMF8Mfrn-",
        "outputId": "096aa26d-ffb3-41b7-a8bb-85fe7ce9e720"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit_learn) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.13.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.8/dist-packages (from scikeras) (23.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import nltk\n",
        "import wandb\n",
        "import os\n",
        "import string\n",
        "import spacy\n",
        "import keras\n",
        "import urllib.request, zipfile\n",
        "from collections import Counter\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from sklearn import preprocessing\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.models import FastText\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from pandas.core.missing import find_valid_index\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "from tabulate import tabulate\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "stemmer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Gx6SH7TOfub4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUlrJbzCfvyc",
        "outputId": "4ad3b26f-2167-4d0e-b86f-cbfedd5b687f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarageorgisplatonas\u001b[0m (\u001b[33mnlp-squad\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_LOG_MODEL=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b8IoQHxfw-G",
        "outputId": "e7077c7b-1615-4609-9c0b-d2726beea4b5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_LOG_MODEL=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "if not os.path.exists('./POS-Tagging'):\n",
        "  os.makedirs('./POS-Tagging')\n",
        "os.chdir('./POS-Tagging')\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "eOPELgnYfyvD",
        "outputId": "4ed029b0-3fb4-447b-84ee-0598437abcf5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-77a02f9a-a4ad-46eb-953d-0df7ca1c9744\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-77a02f9a-a4ad-46eb-953d-0df7ca1c9744\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving en_lines-ud-dev.txt to en_lines-ud-dev.txt\n",
            "Saving en_lines-ud-test.txt to en_lines-ud-test.txt\n",
            "Saving en_lines-ud-train.txt to en_lines-ud-train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL_exeMt2Gxn"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def myData(name, percentage):\n",
        "  with open(name, \"r\", encoding=\"utf-8\") as myfile:\n",
        "    my_data = myfile.read()\n",
        "    cropped_data = my_data[:int(len(my_data)*percentage)]\n",
        "\n",
        "  return cropped_data\n",
        "\n",
        "\n",
        "train_data = myData(\"en_lines-ud-train.txt\", 1) # as a second parameter pass the percentage of the data you want\n",
        "dev_data = myData(\"en_lines-ud-dev.txt\", 1)\n",
        "test_data = myData(\"en_lines-ud-test.txt\", 1)"
      ],
      "metadata": {
        "id": "mWxFKGDhf-pE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('sentencizer')\n",
        "\n",
        "# tokenize and lemmatize the data and finally create x-y list\n",
        "def processing(my_data):\n",
        "  \n",
        "  tuples=[]\n",
        "  x = []\n",
        "  y = []\n",
        "  for sent in my_data.sents:\n",
        "    partial_tuple=[]\n",
        "    temp_x = []\n",
        "    temp_y = []\n",
        "    for token in sent:\n",
        "      if token.pos_:\n",
        "        temp_x.append(token.lemma_)\n",
        "        temp_y.append(token.pos_)\n",
        "        partial_tuple.append((token.lemma_, token.pos_))\n",
        "    tuples.append(partial_tuple)\n",
        "    x.append(temp_x)\n",
        "    y.append(temp_y)\n",
        "  return tuples, x, y\n",
        "\n",
        "train_tuples, x_train, y_train = processing(nlp(train_data))\n",
        "dev_tuples, x_dev, y_dev = processing(nlp(dev_data))\n",
        "test_tuples, x_test, y_test = processing(nlp(test_data))"
      ],
      "metadata": {
        "id": "gwf2elejgAyP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SToZwv8H2XKw"
      },
      "source": [
        "## Loading Pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./embeddings'):\n",
        "  os.makedirs('./embeddings')\n",
        "\n",
        "urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip', './embeddings/wiki-news-300d-1M.vec.zip')\n",
        "zip_ref = zipfile.ZipFile('./embeddings/wiki-news-300d-1M.vec.zip', 'r')\n",
        "zip_ref.extractall('./embeddings')\n",
        "zip_ref.close()\n",
        "\n",
        "embs_path = './embeddings/wiki-news-300d-1M.vec'\n",
        "embeddings = KeyedVectors.load_word2vec_format(embs_path, binary=False)"
      ],
      "metadata": {
        "id": "avpGPUregDGR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTaWRJJf2gs5"
      },
      "source": [
        "## Data Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPSghOH8V0J2"
      },
      "source": [
        "### Word count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the word_index dictionary with an index for the pad token and an index for the oov words\n",
        "word_index = {'pad': 0, 'oov': 1}\n",
        "\n",
        "# Flatten the list of tuples into a list of words\n",
        "all_words = [word for sentence in train_tuples for word, tag in sentence]\n",
        "\n",
        "# Count the number of occurrences of each word\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Create an entry in the word_index dictionary for each unique word, with the index being the current length of the dictionary\n",
        "for word, count in word_counts.items():\n",
        "    word_index[word] = len(word_index)\n",
        "\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMxcO6migMSg",
        "outputId": "d6335746-b568-468f-cff4-6d16f48ce36c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pad': 0, 'oov': 1, 'show': 2, 'all': 3, 'about': 4, 'ANSI': 5, 'SQL': 6, 'query': 7, 'mode': 8, 'some': 9, 'of': 10, 'the': 11, 'content': 12, 'in': 13, 'this': 14, 'topic': 15, 'may': 16, 'not': 17, 'be': 18, '\\n': 19, 'applicable': 20, 'to': 21, 'language': 22, '.': 23, 'you': 24, 'can': 25, 'create': 26, 'one': 27, 'two': 28, ':': 29, 'ANSI-89': 30, 'describe': 31, 'traditional': 32, 'Jet': 33, 'syntax': 34, 'conform': 35, 'closely': 36, 'ansi-89': 37, 'Level': 38, '1': 39, 'specification': 40, ',': 41, 'but': 42, 'level': 43, 'compliant': 44, 'certain': 45, 'feature': 46, 'implement': 47, 'and': 48, 'wildcard': 49, 'character': 50, 'visual': 51, 'Basic': 52, 'for': 53, 'Applications': 54, '(': 55, 'VBA': 56, ')': 57, 'ansi-92': 58, 'provide': 59, 'new': 60, 'reserved': 61, 'word': 62, 'rule': 63, 'that': 64, 'enhance': 65, 'your': 66, 'ability': 67, 'filter': 68, 'statement': 69, 'have': 70, 'more': 71, 'Microsoft': 72, 'Access': 73, '2000': 74, 'use': 75, 'adox': 76, 'could': 77, 'programmatically': 78, 'however': 79, 'any': 80, 'visible': 81, 'Database': 82, 'window': 83, 'because': 84, 'there': 85, 'no': 86, 'option': 87, 'set': 88, 'user': 89, 'interface': 90, 'now': 91, '2002': 92, 'through': 93, 'current': 94, 'database': 95, 'as': 96, 'default': 97, 'why': 98, '?': 99, 'want': 100, 'follow': 101, 'reason': 102, 'anticipate': 103, 'upsize': 104, 'application': 105, 'future': 106, 'an': 107, 'project': 108, 'will': 109, 'run': 110, 'with': 111, 'minimal': 112, 'change': 113, 'a': 114, 'Server': 115, 'take': 116, 'advantage': 117, 'find': 118, 'such': 119, 'security': 120, 'setting': 121, 'by': 122, 'GRANT': 123, 'revoke': 124, 'sql': 125, 'distinct': 126, 'aggregate': 127, 'function': 128, 'reference': 129, 'example': 130, 'sum': 131, '-': 132, 'DISTINCT': 133, 'price': 134, 'LIMIT': 135, 'nn': 136, 'ROWS': 137, 'clause': 138, 'limit': 139, 'number': 140, 'row': 141, 'return': 142, 'should': 143, 'avoid': 144, 'mix': 145, 'under': 146, 'different': 147, 'file': 148, 'format': 149, 'disable': 150, ';': 151, 'only': 152, 'available': 153, 'connection': 154, 'store': 155, 'information': 156, 'data': 157, 'source': 158, 'OLE': 159, 'DB': 160, 'datum': 161, 'associate': 162, 'bind': 163, 'or': 164, 'access': 165, 'page': 166, 'when': 167, 'open': 168, 'read': 169, 'link': 170, 'base': 171, 'on': 172, 'connect': 173, 'appropriate': 174, 'Office': 175, 'Data': 176, 'Connection': 177, '.odc': 178, 'HTML': 179, 'xml': 180, 'view': 181, 'edit': 182, 'text': 183, 'editor': 184, 'Universal': 185, 'Link': 186, '.udl': 187, 'standard': 188, 'Links': 189, 'persistent': 190, 'object': 191, 'definition': 192, 'benefit': 193, 'simplify': 194, 'task': 195, 'deploy': 196, 'relate': 197, 'common': 198, 'share': 199, 'single': 200, 'if': 201, 'move': 202, 'copy': 203, 'instead': 204, 'update': 205, 'ConnectionString': 206, 'property': 207, 'each': 208, 'dependent': 209, 'need': 210, 'make': 211, 'point': 212, 'right': 213, 'location': 214, 'choose': 215, 'whether': 216, 'simply': 217, 'without': 218, 'also': 219, 'exist': 220, 'establish': 221, 'between': 222, \"'s\": 223, 'ConnectionFile': 224, 'name': 225, 'time': 226, 'extract': 227, 'either': 228, 'remember': 229, 'other': 230, 'affect': 231, 'break': 232, 'null': 233, 'automatically': 234, 'convert': 235, 'previous': 236, 'version': 237, 'must': 238, 'before': 239, 'it': 240, 'design': 241, 'backup': 242, 'pagefilename.bak.htm': 243, 'at': 244, 'same': 245, 'original': 246, 'revert': 247, 'delete': 248, 'converted': 249, 'rename': 250, 'then': 251, 'include': 252, 'web': 253, 'component': 254, 'PivotTable': 255, 'list': 256, 'chart': 257, 'spreadsheet': 258, 'valid': 259, 'license': 260, 'able': 261, 'those': 262, 'analyze': 263, 'several': 264, 'technique': 265, 'help': 266, 'crosstab': 267, 'calculate': 268, 'restructure': 269, 'easy': 270, 'analysis': 271, 'Crosstab': 272, 'average': 273, 'count': 274, 'type': 275, 'total': 276, 'group': 277, 'â€“': 278, 'down': 279, 'left': 280, 'side': 281, 'datasheet': 282, 'another': 283, 'across': 284, 'top': 285, 'comparison': 286, 'select': 287, '2': 288, 'add': 289, 'interactive': 290, 'table': 291, 'dynamically': 292, 'from': 293, 'within': 294, 'browser': 295, 'layout': 296, 'they': 297, 'field': 298, 'display': 299, 'column': 300, 'area': 301, 'remove': 302, 'sort': 303, 'illustration': 304, 'order': 305, 'View': 306, 'form': 307, 'build': 308, 'report': 309, 'underlie': 310, 'Component': 311, 'so': 312, 'do': 313, 'thing': 314, 'Employees': 315, 'customize': 316, 'PivotChart': 317, 'control': 318, 'how': 319, 'present': 320, 'let': 321, 'compare': 322, 'summarize': 323, 'value': 324, 'element': 325, 'summary': 326, 'subset': 327, 'drop': 328, 'predefine': 329, 'workspace': 330, 'Detail': 331, 'after': 332, 'caption': 333, 'cover': 334, 'up': 335, 'still': 336, 'drag': 337, 'additional': 338, 'unique': 339, 'item': 340, 'detail': 341, 'record': 342, 'Filter': 343, 'allow': 344, 'confine': 345, 'particular': 346, 'part': 347, 'Product': 348, 'product': 349, 'high': 350, 'low': 351, 'multiple': 352, 'close': 353, 'refer': 354, 'inner': 355, 'outer': 356, 'switch': 357, 'Outer': 358, '3': 359, '4': 360, 'repeat': 361, 'long': 362, 'see': 363, 'differently': 364, 'depend': 365, 'series': 366, 'pie': 367, 'consist': 368, 'additionally': 369, 'write': 370, 'style': 371, 'similar': 372, 'document': 373, 'combination': 374, 'like': 375, 'tag': 376, 'html': 377, 'template': 378, 'specific': 379, 'output': 380, 'note': 381, 'require': 382, 'css': 383, 'xsl': 384, 'sheet': 385, 'Internet': 386, 'Explorer': 387, '5': 388, 'later': 389, 'internet': 390, 'collapsible|expandable': 391, 'tree': 392, 'insure': 393, 'XML': 394, 'intranet': 395, 'Website': 396, 'consistent': 397, 'uniform': 398, 'appearance': 399, 'extensible': 400, 'Stylesheet': 401, 'Language': 402, 'Transformation': 403, 'XSLT': 404, 'currently': 405, 'development': 406, 'World': 407, 'Wide': 408, 'Web': 409, 'Consortium': 410, 'W3C': 411, 'support': 412, 'map': 413, 'transform': 414, 'into': 415, 'way': 416, 'presentation': 417, 'target': 418, 'back': 419, 'again': 420, 'typically': 421, 'developer': 422, 'transformation': 423, 'apply': 424, 'during': 425, 'export': 426, 'interpret': 427, 'recognize': 428, 'Service': 429, 'Advertising': 430, 'Protocol': 431, 'SAP': 432, 'custom': 433, 'purchase': 434, 'many': 435, 'construct': 436, 'structure': 437, 'command': 438, 'programming': 439, 'which': 440, 'variable': 441, 'loop': 442, 'iteration': 443, 'conditional': 444, 'give': 445, 'considerable': 446, 'over': 447, 'convenient': 448, 'readily': 449, 'schema': 450, 'both': 451, 'optionally': 452, 'Schema': 453, 'XSD': 454, 'behind': 455, 'Extensible': 456, 'Style': 457, 'XSL': 458, 'process': 459, '.mdb': 460, 'well': 461, 'produce': 462, '.adp': 463, 'just': 464, 'save': 465, '.xml': 466, 'formal': 467, 'what': 468, '.xsd': 469, 'Customers': 470, 'Orders': 471, 'Details': 472, 'customer': 473, 'call': 474, 'ReportML': 475, 'model': 476, '_': 477, 'report.xml': 478, 'addition': 479, 'specify': 480, '.xsl': 481, 'unavailable': 482, '\"': 483, 'develop': 484, 'event': 485, 'attribute': 486, 'mean': 487, 'basic': 488, 'root': 489, 'encompass': 490, 'entire': 491, 'start': 492, 'end': 493, 'match': 494, 'case': 495, 'sensitive': 496, 'corresponding': 497, 'empty': 498, 'denote': 499, 'special': 500, 'shorthand': 501, 'out': 502, 'overlap': 503, 'properly': 504, 'nest': 505, 'reserve': 506, 'themselves': 507, 'portion': 508, 'substitute': 509, 'sequence': 510, 'entity': 511, 'Character': 512, 'Entity': 513, 'where': 514, 'declaration': 515, 'state': 516, 'optional': 517, 'recommend': 518, 'together': 519, 'notice': 520, '&': 521, 'amp;apo': 522, 'apostrophe': 523, 'import': 524, 'receiving': 525, 'purpose': 526, 'misinterpret': 527, 'directly': 528, 'Mom': 529, 'Boston': 530, 'Crab': 531, 'Meat': 532, 'white': 533, 'space': 534, 'throughout': 535, 'readability': 536, 'while': 537, 'consider': 538, 'define': 539, 'Excel': 540, 'familiar': 541, 'work': 542, 'accomplish': 543, 'modify': 544, 'prepare': 545, 'toolbar': 546, 'reflect': 547, 'hide': 548, 'cell': 549, 'might': 550, 'difference': 551, 'program': 552, 'noninteractive': 553, 'copying': 554, 'FrontPage': 555, 'than': 556, 'Word': 557, 'unformatted': 558, 'filtering': 559, 'Filtering': 560, 'autofiltering': 561, 'first': 562, 'Sport': 563, 'Golf': 564, 'sale': 565, 'Quarter': 566, 'Qtr3': 567, 'arrow': 568, 'Field': 569, 'blue': 570, 'black': 571, 'AutoFilter': 572, 'button': 573, 'particularly': 574, 'helpful': 575, 'large': 576, 'amount': 577, 'focus': 578, 'category': 579, 'Region': 580, 'South': 581, 'West': 582, 'region': 583, 'Conditional': 584, 'bottom': 585, 'n': 586, 'three': 587, 'city': 588, 'generate': 589, 'most': 590, 'five': 591, 'least': 592, 'profitable': 593, 'percentage': 594, '25': 595, '%': 596, 'salesperson': 597, 'handle': 598, '40': 599, '10': 600, 'people': 601, 'Chart': 602, 'far': 603, 'narrow': 604, 'selection': 605, 'useful': 606, 'contain': 607, 'axis': 608, 'interior': 609, 'horizontal': 610, 'line': 611, 'upward': 612, 'distance': 613, 'near': 614, 'parallel': 615, 'below': 616, 'remain': 617, 'constant': 618, 'perpendicular': 619, 'above': 620, 'contract': 621, 'downward': 622, 'lengthen': 623, 'exterior': 624, 'away': 625, 'center': 626, 'grid': 627, 'grow': 628, 'towards': 629, 'small': 630, 'grouped': 631, 'press': 632, 'CTRL': 633, 'C.': 634, 'Press': 635, 'v': 636, 'paste': 637, 'vertical': 638, 'halfway': 639, 'locate': 640, 'recreate': 641, 'Null': 642, 'calculation': 643, 'blank': 644, 'their': 645, 'Count': 646, 'Nullvalues': 647, 'nz': 648, 'zero': 649, 'expression': 650, 'installing': 651, 'licensing': 652, 'distribute': 653, 'Components': 654, 'interact': 655, 'interactivity': 656, 'is': 657, 'formatting': 658, 'List': 659, 'protect': 660, 'these': 661, 'environment': 662, 'who': 663, 'print': 664, 'manipulate': 665, 'functionality': 666, \"'ve\": 667, 'instal': 668, 'separately': 669, 'installation': 670, 'designer': 671, 'prompt': 672, 'install': 673, 'configure': 674, 'pointer': 675, 'Resource': 676, 'Kit': 677, 'once': 678, 'computer': 679, 'Users': 680, 'full': 681, 'capability': 682, 'office': 683, 'site': 684, 'organization': 685, 'enterprise': 686, 'agreement': 687, 'download': 688, 'package': 689, '.lpk': 690, 'License': 691, 'Package': 692, 'author': 693, 'Tool': 694, 'MSDN': 695, 'Online': 696, 'mixing': 697, 'compatible': 698, 'decide': 699, 'go': 700, 'runtime': 701, 'error': 702, 'unexpected': 703, 'result': 704, 'range': 705, 'criterion': 706, 'asterisk': 707, 'country': 708, 'u': 709, 'country|region': 710, 'countries|region': 711, 'begin': 712, 'letter': 713, 'percent': 714, 'sign': 715, 'duplicate': 716, 'alias': 717, 'ambiguous': 718, 'problem': 719, 'cause': 720, 'general': 721, 'following': 722, 'prevent': 723, '97': 724, 'retest': 725, 'ensure': 726, 'expect': 727, 'rewrite': 728, 'necessary': 729, 'accidental': 730, 'intentional': 731, 'code': 732, 'changing': 733, 'confusion': 734, 'always': 735, 'search': 736, 'librarie': 737, 'library': 738, 'dialog': 739, 'box': 740, 'References': 741, 'Tools': 742, 'menu': 743, 'Editor': 744, 'specified': 745, 'reflibpath': 746, 'key': 747, 'Windows': 748, 'Registry': 749, 'check': 750, 'existence': 751, 'matching': 752, 'load': 753, 'path': 754, 'folder': 755, 'Msaccess.exe': 756, 'system': 757, 'System': 758, 'System32': 759, 'WINNT': 760, 'PATH': 761, 'Help': 762, 'subfolder': 763, 'perform': 764, 'fix': 765, 'manually': 766, 'procedure': 767, 'enable': 768, 'quickly': 769, 'review': 770, 'enter': 771, 'look': 772, 'Save': 773, 'File': 774, 'carry': 775, 'determine': 776, 'outcome': 777, 'Datasheet': 778, 'arrange': 779, 'implication': 780, 'leave': 781, 'unbound': 782, 'subform': 783, 'subreport': 784, 'appear': 785, 'header': 786, 'footer': 787, 'section': 788, 'Controls': 789, 'Form': 790, 'place': 791, 'outermost': 792, 'Report': 793, 'navigation': 794, 'comment': 795, 'block': 796, 'picture': 797, 'bitmap': 798, 'image': 799, 'AllowAdditions': 800, 'AllowDeletions': 801, 'AllowEdits': 802, 'False': 803, 'ignore': 804, '\\n\\n': 805, 'wrong': 806, 'telephone': 807, 'ring': 808, 'dead': 809, 'night': 810, 'voice': 811, 'ask': 812, 'someone': 813, 'he': 814, 'much': 815, 'think': 816, 'happen': 817, 'would': 818, 'conclude': 819, 'nothing': 820, 'real': 821, 'except': 822, 'chance': 823, 'beginning': 824, 'its': 825, 'consequence': 826, 'turn': 827, 'predetermine': 828, 'come': 829, 'stranger': 830, 'mouth': 831, 'question': 832, 'story': 833, 'itself': 834, 'something': 835, 'tell': 836, 'Quinn': 837, 'little': 838, 'detain': 839, 'we': 840, 'great': 841, 'importance': 842, 'know': 843, 'thirty': 844, 'year': 845, 'old': 846, 'marry': 847, 'father': 848, 'his': 849, 'wife': 850, 'son': 851, 'book': 852, 'precise': 853, 'mystery': 854, 'novel': 855, 'William': 856, 'Wilson': 857, 'rate': 858, 'bring': 859, 'enough': 860, 'money': 861, 'live': 862, 'modestly': 863, 'New': 864, 'York': 865, 'apartment': 866, 'spend': 867, 'six': 868, 'month': 869, 'rest': 870, 'free': 871, 'wish': 872, 'painting': 873, 'movie': 874, 'summer': 875, 'watch': 876, 'baseball': 877, 'television': 878, 'winter': 879, 'opera': 880, 'More': 881, 'anything': 882, 'walk': 883, 'nearly': 884, 'every': 885, 'day': 886, 'rain': 887, 'shine': 888, 'hot': 889, 'cold': 890, 'never': 891, 'really': 892, 'anywhere': 893, 'wherever': 894, 'leg': 895, 'inexhaustible': 896, 'labyrinth': 897, 'endless': 898, 'step': 899, 'matter': 900, 'neighborhood': 901, 'street': 902, 'feeling': 903, 'lose': 904, 'himself': 905, 'feel': 906, 'though': 907, 'movement': 908, 'reduce': 909, 'eye': 910, 'escape': 911, 'obligation': 912, 'else': 913, 'measure': 914, 'peace': 915, 'salutatory': 916, 'emptiness': 917, 'world': 918, 'outside': 919, 'around': 920, 'speed': 921, 'keep': 922, 'impossible': 923, 'dwell': 924, 'very': 925, 'Motion': 926, 'essence': 927, 'act': 928, 'put': 929, 'foot': 930, 'front': 931, 'drift': 932, 'own': 933, 'body': 934, 'wander': 935, 'aimlessly': 936, 'become': 937, 'equal': 938, 'good': 939, 'nowhere': 940, 'realize': 941, 'intention': 942, 'ever': 943, 'past': 944, 'ambitious': 945, 'young': 946, 'man': 947, 'publish': 948, 'poetry': 949, 'play': 950, 'critical': 951, 'essay': 952, 'translation': 953, 'quite': 954, 'abruptly': 955, 'die': 956, 'friend': 957, 'haunt': 958, 'although': 959, 'continue': 960, 'anyone': 961, 'next': 962, 'morning': 963, 'wake': 964, 'early': 965, 'week': 966, 'drink': 967, 'coffee': 968, 'butter': 969, 'toast': 970, 'score': 971, 'paper': 972, 'Mets': 973, 'ninth': 974, 'inning': 975, 'occur': 976, 'appointment': 977, 'even': 978, 'locution': 979, \"'\": 980, 'seem': 981, 'odd': 982, 'Paul': 983, 'Auster': 984, 'person': 985, 'idea': 986, 'nevertheless': 987, 'wear': 988, 'imitation': 989, 'clear': 990, 'breakfast': 991, 'dish': 992, 'toss': 993, 'newspaper': 994, 'couch': 995, 'bathroom': 996, 'shower': 997, 'shave': 998, 'bedroom': 999, 'wrap': 1000, 'towel': 1001, 'closet': 1002, 'pick': 1003, 'clothe': 1004, 'tend': 1005, 'toward': 1006, 'jacket': 1007, 'tie': 1008, 'since': 1009, 'funeral': 1010, 'hang': 1011, 'amidst': 1012, 'debris': 1013, 'wardrobe': 1014, 'dismiss': 1015, 'shirt': 1016, 'too': 1017, 'gray': 1018, 'red': 1019, 'affair': 1020, 'kind': 1021, 'trance': 1022, 'until': 1023, 'hand': 1024, 'doorknob': 1025, 'suspect': 1026, 'I': 1027, 'say': 1028, 'exactly': 1029, 'hour': 1030, 'climb': 1031, 'bus': 1032, '70th': 1033, 'Street': 1034, 'Fifth': 1035, 'Avenue': 1036, 'answer': 1037, 'park': 1038, 'green': 1039, 'sun': 1040, 'sharp': 1041, 'fleeting': 1042, 'shadow': 1043, 'Frick': 1044, 'austere': 1045, 'abandon': 1046, 'moment': 1047, 'Vermeer': 1048, 'Soldier': 1049, 'Young': 1050, 'Girl': 1051, 'Smiling': 1052, 'try': 1053, 'girl': 1054, 'face': 1055, 'exact': 1056, 'position': 1057, 'her': 1058, 'cup': 1059, 'faceless': 1060, 'mind': 1061, 'catch': 1062, 'glimpse': 1063, 'wall': 1064, 'sunlight': 1065, 'pour': 1066, 'surround': 1067, 'cross': 1068, 'eastward': 1069, 'Madison': 1070, 'south': 1071, 'arrive': 1072, 'stand': 1073, 'building': 1074, 'pause': 1075, 'suddenly': 1076, 'anymore': 1077, 'remarkably': 1078, 'calm': 1079, 'everything': 1080, 'already': 1081, 'door': 1082, 'lead': 1083, 'lobby': 1084, 'last': 1085, 'advice': 1086, 'my': 1087, 'woman': 1088, 'throw': 1089, 'off': 1090, 'track': 1091, 'fast': 1092, 'absorb': 1093, 'presence': 1094, 'she': 1095, 'impression': 1096, 'talk': 1097, 'force': 1098, 'respond': 1099, 'therefore': 1100, 'ground': 1101, 'fall': 1102, 'manage': 1103, 'piece': 1104, 'encounter': 1105, 'memory': 1106, 'tendency': 1107, 'subvert': 1108, 'sure': 1109, 'perhaps': 1110, 'height': 1111, 'hip': 1112, 'touch': 1113, 'wide': 1114, 'voluptuous': 1115, 'dark': 1116, 'hair': 1117, 'self': 1118, 'vaguely': 1119, 'seductive': 1120, 'dress': 1121, 'lipstick': 1122, 'Mr': 1123, 'tentative': 1124, 'smile': 1125, 'questioning': 1126, 'tilt': 1127, 'head': 1128, 'Virginia': 1129, 'Stillman': 1130, 'Peter': 1131, 'wait': 1132, 'eight': 1133, \"o'clock\": 1134, 'ten': 1135, 'glance': 1136, 'frantic': 1137, 'explain': 1138, 'threshold': 1139, 'brain': 1140, 'shut': 1141, 'somehow': 1142, 'beyond': 1143, 'loom': 1144, 'blur': 1145, 'room': 1146, 'richly': 1147, 'furnish': 1148, 'numerous': 1149, 'art': 1150, 'silver': 1151, 'ashtray': 1152, 'elaborately': 1153, 'frame': 1154, 'sit': 1155, 'sofa': 1156, 'alone': 1157, 'living': 1158, 'Mrs.': 1159, 'husband': 1160, 'surely': 1161, 'minute': 1162, 'light': 1163, 'almost': 1164, 'noon': 1165, 'consult': 1166, 'smell': 1167, 'perfume': 1168, 'hover': 1169, 'imagine': 1170, 'Max': 1171, 'Work': 1172, 'cigarette': 1173, 'blow': 1174, 'smoke': 1175, 'please': 1176, 'gust': 1177, 'disperse': 1178, 'hear': 1179, 'sound': 1180, 'entirely': 1181, 'blond': 1182, 'child': 1183, 'uncannily': 1184, 'thought': 1185, 'vanish': 1186, 'velvet': 1187, 'armchair': 1188, 'opposite': 1189, 'seat': 1190, 'nor': 1191, 'acknowledge': 1192, 'attention': 1193, 'immobility': 1194, 'manner': 1195, 'speak': 1196, 'phone': 1197, 'machine': 1198, 'fitful': 1199, 'alternate': 1200, 'slow': 1201, 'rapid': 1202, 'gesture': 1203, 'rigid': 1204, 'yet': 1205, 'expressive': 1206, 'operation': 1207, 'lie': 1208, 'relearn': 1209, 'motion': 1210, 'conscious': 1211, 'submovement': 1212, 'flow': 1213, 'spontaneity': 1214, 'marionette': 1215, 'string': 1216, 'White': 1217, 'neck': 1218, 'pant': 1219, 'shoe': 1220, 'sock': 1221, 'against': 1222, 'pallor': 1223, 'skin': 1224, 'flaxen': 1225, 'thinness': 1226, 'effect': 1227, 'transparent': 1228, 'vein': 1229, 'milky': 1230, 'dissolve': 1231, 'mixture': 1232, 'sky': 1233, 'cloud': 1234, 'address': 1235, 'silent': 1236, 'settle': 1237, 'slowly': 1238, 'chair': 1239, 'meet': 1240, 'invisible': 1241, 'blind': 1242, 'possible': 1243, 'study': 1244, 'recognition': 1245, 'flicker': 1246, 'hold': 1247, 'stare': 1248, 'dumbly': 1249, 'pass': 1250, 'yes': 1251, 'thank': 1252, 'course': 1253, 'here': 1254, 'corner': 1255, '72nd': 1256, 'wave': 1257, 'cab': 1258, 'car': 1259, 'rattle': 1260, 'Side': 1261, 'wonder': 1262, 'air': 1263, 'house': 1264, 'hungry': 1265, 'eat': 1266, 'strange': 1267, 'correct': 1268, 'fourteen': 1269, 'stay': 1270, 'four': 1271, 'shrug': 1272, 'discrepancy': 1273, 'learn': 1274, 'often': 1275, 'retrace': 1276, 'along': 1277, '107th': 1278, 'Broadway': 1279, 'uptown': 1280, 'suitable': 1281, 'bar': 1282, 'appeal': 1283, 'tonight': 1284, 'boozy': 1285, 'chatter': 1286, 'normally': 1287, 'welcome': 1288, '112th': 1289, 'Heights': 1290, 'Luncheonette': 1291, 'brightly': 1292, 'dreary': 1293, 'rack': 1294, 'girlie': 1295, 'magazine': 1296, 'stationery': 1297, 'supply': 1298, 'patron': 1299, 'Formica': 1300, 'counter': 1301, 'swivel': 1302, 'stool': 1303, 'tall': 1304, 'puerto': 1305, 'rican': 1306, 'cardboard': 1307, 'chef': 1308, 'hat': 1309, 'job': 1310, 'food': 1311, 'mainly': 1312, 'gristle': 1313, 'stud': 1314, 'hamburger': 1315, 'patty': 1316, 'bland': 1317, 'sandwich': 1318, 'pale': 1319, 'tomato': 1320, 'wilt': 1321, 'lettuce': 1322, 'milkshake': 1323, 'egg': 1324, 'cream': 1325, 'bun': 1326, 'ensconce': 1327, 'cash': 1328, 'register': 1329, 'boss': 1330, 'bald': 1331, 'curly': 1332, 'concentration': 1333, 'camp': 1334, 'tattoe': 1335, 'forearm': 1336, 'lord': 1337, 'domain': 1338, 'pipe': 1339, 'cigar': 1340, 'impassively': 1341, 'owl': 1342, 'edition': 1343, 'Daily': 1344, 'News': 1345, 'deserted': 1346, 'seven': 1347, 'buttered': 1348, 'roll': 1349, 'lap': 1350, 'glass': 1351, 'hotel': 1352, 'brown': 1353, 'overcoat': 1354, 'fashioned': 1355, 'carpet': 1356, 'bag': 1357, 'routine': 1358, 'vary': 1359, 'advance': 1360, 'sometimes': 1361, 'mere': 1362, 'increment': 1363, 'pausing': 1364, 'weigh': 1365, 'among': 1366, 'difficult': 1367, 'briskly': 1368, 'stop': 1369, 'shuffle': 1370, 'strain': 1371, 'rhythm': 1372, 'disrupt': 1373, 'hare': 1374, 'pursuit': 1375, 'tortoise': 1376, 'remind': 1377, 'dutifully': 1378, 'notebook': 1379, 'meaning': 1380, 'elude': 1381, 'narrowly': 1382, 'circumscribed': 1383, 'bound': 1384, 'north': 1385, 'Riverside': 1386, 'Park': 1387, 'east': 1388, 'Amsterdam': 1389, 'haphazard': 1390, 'journey': 1391, 'itinerary': 1392, 'border': 1393, 'precision': 1394, 'baffle': 1395, 'respect': 1396, 'aimless': 1397, 'permanently': 1398, 'pavement': 1399, 'indeed': 1400, 'stoop': 1401, 'examine': 1402, 'archeologist': 1403, 'inspect': 1404, 'shard': 1405, 'prehistoric': 1406, 'ruin': 1407, 'occasionally': 1408, 'pore': 1409, 'onto': 1410, 'sidewalk': 1411, 'gently': 1412, 'inside': 1413, 'reach': 1414, 'coat': 1415, 'pocket': 1416, 'having': 1417, 'complete': 1418, 'collect': 1419, 'valueless': 1420, 'broken': 1421, 'discard': 1422, 'stray': 1423, 'bit': 1424, 'junk': 1425, 'collapsible': 1426, 'umbrella': 1427, 'shorn': 1428, 'material': 1429, 'sever': 1430, 'rubber': 1431, 'doll': 1432, 'glove': 1433, 'shatter': 1434, 'bulb': 1435, 'sogged': 1436, 'shred': 1437, 'torn': 1438, 'photograph': 1439, 'anonymous': 1440, 'machinery': 1441, 'sundry': 1442, 'clump': 1443, 'flotsam': 1444, 'identify': 1445, 'fact': 1446, 'scavenging': 1447, 'seriously': 1448, 'intrigue': 1449, 'observe': 1450, 'stupidly': 1451, 'surface': 1452, 'secret': 1453, 'accumulate': 1454, 'plot': 1455, 'various': 1456, 'stratagem': 1457, 'steal': 1458, 'somewhere': 1459, 'meal': 1460, 'bump': 1461, 'mumble': 1462, 'apology': 1463, 'neither': 1464, 'happy': 1465, 'sad': 1466, 'twice': 1467, 'scavenge': 1468, 'haul': 1469, 'unusually': 1470, 'middle': 1471, 'remerge': 1472, 'few': 1473, 'methodically': 1474, 'macadam': 1475, 'footpath': 1476, 'thrash': 1477, 'bush': 1478, 'stick': 1479, 'quest': 1480, 'abate': 1481, 'greenery': 1482, 'stone': 1483, 'twig': 1484, 'dry': 1485, 'dog': 1486, 'turd': 1487, 'sniff': 1488, 'carefully': 1489, 'afternoon': 1490, 'lunch': 1491, 'bench': 1492, 'gaze': 1493, 'Hudson': 1494, 'warm': 1495, 'sprawl': 1496, 'grass': 1497, 'asleep': 1498, 'darkness': 1499, 'dinner': 1500, 'Apollo': 1501, 'Coffee': 1502, 'Shop': 1503, '97th': 1504, 'contact': 1505, 'confirm': 1506, 'whom': 1507, 'home': 1508, 'essential': 1509, 'involved': 1510, 'cut': 1511, 'embark': 1512, 'meaningless': 1513, 'merely': 1514, 'bide': 1515, 'lull': 1516, 'lethargy': 1517, 'strike': 1518, 'assume': 1519, 'aware': 1520, 'unlikely': 1521, 'discreet': 1522, 'blend': 1523, 'traffic': 1524, 'drastic': 1525, 'trouble': 1526, 'discover': 1527, 'watcher': 1528, 'certainty': 1529, 'replace': 1530, 'situation': 1531, 'comfort': 1532, 'believe': 1533, 'belief': 1534, 'waste': 1535, 'actually': 1536, 'interpretation': 1537, 'knowledge': 1538, 'accept': 1539, 'article': 1540, 'faith': 1541, 'occupy': 1542, 'excursion': 1543, 'teach': 1544, 'understand': 1545, 'connectedness': 1546, 'reversal': 1547, 'thus': 1548, 'usurp': 1549, 'sovereignty': 1550, 'inwardness': 1551, 'flood': 1552, 'external': 1553, 'drown': 1554, 'exert': 1555, 'degree': 1556, 'fit': 1557, 'despair': 1558, 'Wandering': 1559, 'mindlessness': 1560, 'stagger': 1561, 'blindman': 1562, 'spot': 1563, 'privilege': 1564, 'deny': 1565, 'oblige': 1566, 'concentrate': 1567, 'soon': 1568, 'thereafter': 1569, 'suit': 1570, 'constantly': 1571, 'danger': 1572, 'quicken': 1573, 'pace': 1574, 'crash': 1575, 'guard': 1576, 'mishap': 1577, 'devise': 1578, 'method': 1579, 'deceleration': 1580, 'Daniel': 1581, 'comfortably': 1582, 'stricture': 1583, 'husk': 1584, 'speck': 1585, 'punctuation': 1586, 'mark': 1587, 'brick': 1588, 'life': 1589, 'nightmare': 1590, 'probability': 1591, 'clue': 1592, 'backtrack': 1593, 'predict': 1594, 'theory': 1595, 'behavior': 1596, 'obscure': 1597, 'hint': 1598, 'extreme': 1599, 'suggest': 1600, 'get': 1601, 'unlisted': 1602, 'eliminate': 1603, 'disturbing': 1604, 'temporarily': 1605, 'fail': 1606, 'altogether': 1607, 'bad': 1608, 'identity': 1609, 'important': 1610, 'circumstance': 1611, 'hiring': 1612, 'grant': 1613, 'fill': 1614, 'leap': 1615, 'detective': 1616, 'Stillmans': 1617, 'clean': 1618, 'breast': 1619, 'forgive': 1620, 'yellow': 1621, 'Detective': 1622, 'Agency': 1623, 'listing': 1624, 'Manhattan': 1625, 'Drive': 1626, 'mention': 1627, 'agency': 1628, 'necessarily': 1629, 'advertise': 1630, 'dial': 1631, 'conversation': 1632, 'risk': 1633, 'brush': 1634, 'west': 1635, 'tiny': 1636, 'shift': 1637, 'seep': 1638, 'half': 1639, 'thread': 1640, 'whatever': 1641, 'accustomed': 1642, 'freedom': 1643, 'shuffling': 1644, 'spell': 1645, '116th': 1646, '119th': 1647, 'Streets': 1648, 'Church': 1649, 'Grant': 1650, 'Tomb': 1651, 'polished': 1652, 'bourgeois': 1653, 'sobriety': 1654, 'eleventh': 1655, 'floor': 1656, 'buzzer': 1657, 'intercom': 1658, 'push': 1659, 'ride': 1660, 'elevator': 1661, 'fellow': 1662, 'mid': 1663, 'rumpled': 1664, 'beard': 1665, 'thumb': 1666, 'finger': 1667, 'uncapped': 1668, 'fountain': 1669, 'pen': 1670, 'poise': 1671, 'writing': 1672, 'surprised': 1673, 'standing': 1674, 'tentatively': 1675, 'polite': 1676, 'tone': 1677, 'muster': 1678, 'sorry': 1679, 'disturb': 1680, 'apologize': 1681, 'hardly': 1682, 'myself': 1683, 'honest': 1684, 'complicated': 1685, 'afraid': 1686, 'abstractedly': 1687, 'mutter': 1688, 'hard': 1689, 'dredge': 1690, 'poet': 1691, 'poem': 1692, 'ago': 1693, 'title': 1694, 'Unfinished': 1695, 'Business': 1696, 'hope': 1697, 'pleasant': 1698, 'oddly': 1699, 'shape': 1700, 'corridor': 1701, 'cluttered': 1702, 'everywhere': 1703, 'artist': 1704, 'toy': 1705, 'scatter': 1706, 'truck': 1707, 'bear': 1708, 'monster': 1709, 'fray': 1710, 'upholstered': 1711, 'kitchen': 1712, 'fetch': 1713, 'beer': 1714, 'bottle': 1715, 'wooden': 1716, 'crate': 1717, 'serve': 1718, 'literary': 1719, 'successful': 1720, 'ploy': 1721, 'mirror': 1722, 'Don': 1723, 'Quixote': 1724, 'madness': 1725, 'absurd': 1726, 'ludicrous': 1727, 'delusion': 1728, 'finally': 1729, 'twist': 1730, 'mad': 1731, 'pretend': 1732, 'orchestrate': 1733, 'whole': 1734, 'preoccupy': 1735, 'posterity': 1736, 'accurately': 1737, 'chronicler': 1738, 'adventure': 1739, 'imply': 1740, 'beforehand': 1741, 'Sancho': 1742, 'Panza': 1743, 'faithful': 1744, 'squire': 1745, 'role': 1746, 'destine': 1747, 'engineer': 1748, 'Benengali': 1749, 'quartet': 1750, 'probably': 1751, 'translate': 1752, 'arabic': 1753, 'manuscript': 1754, 'Spanish': 1755, 'skilled': 1756, 'disguise': 1757, 'darken': 1758, 'don': 1759, 'Moor': 1760, 'scene': 1761, 'marketplace': 1762, 'Toledo': 1763, 'cervante': 1764, 'hire': 1765, 'decipher': 1766, 'beauty': 1767, 'tranquil': 1768, 'engage': 1769, 'elaborate': 1770, 'hoax': 1771, 'interesting': 1772, 'opinion': 1773, 'conduct': 1774, 'experiment': 1775, 'test': 1776, 'gullibility': 1777, 'utmost': 1778, 'conviction': 1779, 'spew': 1780, 'nonsense': 1781, 'windmill': 1782, 'knight': 1783, 'barber': 1784, 'basin': 1785, 'helmet': 1786, 'puppet': 1787, 'persuade': 1788, 'agree': 1789, 'extent': 1790, 'tolerate': 1791, 'blasphemy': 1792, 'amusement': 1793, 'obvious': 1794, 'proof': 1795, 'highly': 1796, 'amusing': 1797, 'amuse': 1798, 'lean': 1799, 'ironic': 1800, 'pleaure': 1801, 'obviously': 1802, 'enjoy': 1803, 'nature': 1804, 'pleasure': 1805, 'soundless': 1806, 'laughter': 1807, 'joke': 1808, 'short': 1809, 'punchline': 1810, 'generalize': 1811, 'mirth': 1812, 'response': 1813, 'interrupt': 1814, 'clattering': 1815, 'slamming': 1816, 'burst': 1817, 'perk': 1818, 'rise': 1819, 'excuse': 1820, 'hallway': 1821, 'staccato': 1822, 'shrapnel': 1823, 'basso': 1824, 'rumble': 1825, 'guffaw': 1826, 'daddy': 1827, '!': 1828, 'perfectly': 1829, 'okay': 1830, 'hall': 1831, 'shoot': 1832, 'sight': 1833, 'haired': 1834, 'boy': 1835, 'rapidly': 1836, 'withdraw': 1837, 'shyness': 1838, 'faint': 1839, 'hello': 1840, 'yoyo': 1841, 'exaggerated': 1842, 'pantomine': 1843, 'dunno': 1844, 'Siri': 1845, 'breathing': 1846, 'beside': 1847, 'plastic': 1848, 'artifact': 1849, 'age': 1850, 'fasten': 1851, 'flute': 1852, 'whistling': 1853, 'descend': 1854, 'spark': 1855, 'gasp': 1856, 'dangle': 1857, 'philosopher': 1858, 'rewinde': 1859, 'spool': 1860, 'attempt': 1861, 'brief': 1862, 'thin': 1863, 'blonde': 1864, 'radiantly': 1865, 'beautiful': 1866, 'energy': 1867, 'happiness': 1868, 'taunt': 1869, 'envy': 1870, 'rage': 1871, 'lacerate': 1872, 'pity': 1873, 'spout': 1874, 'drivel': 1875, 'yoyos': 1876, 'ham': 1877, 'omelette': 1878, 'pray': 1879, 'deliverance': 1880, 'laugh': 1881, 'Everybody': 1882, 'shout': 1883, 'spread': 1884, 'arm': 1885, 'spin': 1886, 'gyroscope': 1887, 'glad': 1888, 'extend': 1889, 'shake': 1890, 'uncanny': 1891, 'slenderness': 1892, 'bone': 1893, 'norwegian': 1894, 'Norway': 1895, 'indirectly': 1896, 'Northfield': 1897, 'Minnesota': 1898, 'collapse': 1899, 'send': 1900, 'stride': 1901, 'June': 1902, 'second': 1903, 'tomorrow': 1904, 'third': 1905, 'fourth': 1906, 'debate': 1907, 'forget': 1908, 'trip': 1909, 'Paris': 1910, 'curious': 1911, 'shade': 1912, 'dinginess': 1913, 'laspe': 1914, 'fruit': 1915, 'paint': 1916, 'exhaust': 1917, 'encroach': 1918, 'soot': 1919, 'plaster': 1920, 'crumble': 1921, 'dirty': 1922, 'wash': 1923, 'water': 1924, 'sink': 1925, 'lather': 1926, 'blade': 1927, 'scrape': 1928, 'unpleasant': 1929, 'fart': 1930, 'bowl': 1931, 'cornflake': 1932, 'argument': 1933, 'etiquette': 1934, 'fair': 1935, 'disappear': 1936, 'acceptable': 1937, 'busy': 1938, 'dialing': 1939, 'waiting': 1940, 'operator': 1941, 'charge': 1942, 'cent': 1943, 'crackling': 1944, 'wire': 1945, 'further': 1946, 'possibility': 1947, 'hook': 1948, 'innings': 1949, 'game': 1950, 'St.': 1951, 'Louis': 1952, 'infield': 1953, 'sacrifice': 1954, 'fly': 1955, 'double': 1956, 'Youngblood': 1957, 'care': 1958, 'commercial': 1959, 'twentieth': 1960, 'desk': 1961, 'steadily': 1962, 'bother': 1963, 'signal': 1964, 'slam': 1965, 'receiver': 1966, 'crack': 1967, 'bed': 1968, 'dream': 1969, 'strict': 1970, 'flight': 1971, 'Israel': 1972, 'frisk': 1973, 'electronic': 1974, 'hoop': 1975, 'fore': 1976, 'aft': 1977, 'luggage': 1978, 'patient': 1979, 'visibility': 1980, 'queue': 1981, 'poor': 1982, 'Hasidim': 1983, 'broad': 1984, 'sidelock': 1985, 'fringe': 1986, 'Heathrow': 1987, 'restless': 1988, 'rush': 1989, 'gesticulate': 1990, 'exclaim': 1991, 'jump': 1992, 'hundred': 1993, 'attend': 1994, 'circumcision': 1995, 'firstborn': 1996, 'spiritual': 1997, 'leader': 1998, 'Belzer': 1999, 'Rabbi': 2000, '747': 2001, 'Alexandra': 2002, 'enfilade': 2003, 'hairy': 2004, 'ambush': 2005, 'foreign': 2006, 'lock': 2007, 'childhood': 2008, 'revisit': 2009, 'tallith': 2010, 'katan': 2011, 'scapular': 2012, 'mine': 2013, 'scrap': 2014, 'calico': 2015, 'whereas': 2016, 'theirs': 2017, 'linen': 2018, 'God': 2019, 'instruct': 2020, 'Moses': 2021, 'bid': 2022, 'garment': 2023, 'thousand': 2024, 'our': 2025, 'rear': 2026, 'aircraft': 2027, 'yiddish': 2028, 'certainly': 2029, 'dislike': 2030, 'rather': 2031, 'Hasid': 2032, 'late': 2033, 'twenty': 2034, 'pimply': 2035, 'goggle': 2036, 'underlip': 2037, 'extrude': 2038, 'civilized': 2039, 'impulse': 2040, 'inferior': 2041, 'permit': 2042, 'unrelated': 2043, 'communicate': 2044, 'deal': 2045, 'hearted': 2046, 'visibly': 2047, 'vividly': 2048, 'dodge': 2049, 'aisle': 2050, 'visit': 2051, 'impatiently': 2052, 'lavatory': 2053, 'amiable': 2054, 'geese': 2055, 'pay': 2056, 'English': 2057, 'stewardess': 2058, 'furious': 2059, 'hostess': 2060, 'receive': 2061, 'cry': 2062, 'irritation': 2063, 'retreat': 2064, 'merry': 2065, 'minded': 2066, 'exulting': 2067, 'gentile': 2068, 'uniformed': 2069, 'female': 2070, 'attendant': 2071, 'exotic': 2072, 'bediener': 2073, 'bodyless': 2074, 'difficulty': 2075, 'kosher': 2076, 'big': 2077, 'british': 2078, 'affront': 2079, 'bosom': 2080, 'indignation': 2081, 'Rome': 2082, 'Amused': 2083, 'chicken': 2084, 'kid': 2085, 'British': 2086, 'Airways': 2087, 'chill': 2088, 'death': 2089, 'upon': 2090, 'exercise': 2091, 'recoil': 2092, 'tray': 2093, 'Yiddish': 2094, 'offend': 2095, 'slap': 2096, 'Jew': 2097, 'awful': 2098, 'womenfolk': 2099, 'pack': 2100, 'beef': 2101, 'Jewish': 2102, 'Rumanian': 2103, 'shock': 2104, 'jewish': 2105, 'upbringing': 2106, 'favor': 2107, 'condition': 2108, 'trephena': 2109, 'promise': 2110, 'duty': 2111, 'listen': 2112, 'proposition': 2113, 'prepared': 2114, 'fifteen': 2115, 'dollar': 2116, 'generous': 2117, 'earn': 2118, 'hasidic': 2119, 'sweater': 2120, 'factory': 2121, 'Jersey': 2122, 'rabbi': 2123, 'Jerusalem': 2124, 'expensive': 2125, 'disagreeable': 2126, 'guiltily': 2127, 'appetite': 2128, 'spoil': 2129, 'prayer': 2130, 'fervent': 2131, 'discomfiture': 2132, 'Minchah': 2133, 'service': 2134, 'rock': 2135, 'stretch': 2136, 'Jews': 2137, 'lively': 2138, 'childlike': 2139, 'cheerful': 2140, 'natural': 2141, 'love': 2142, 'costume': 2143, 'sell': 2144, 'outsider': 2145, 'learning': 2146, 'lecture': 2147, 'Hebrew': 2148, 'University': 2149, 'mathematician': 2150, 'puzzled': 2151, 'astonish': 2152, 'innocent': 2153, 'ignorant': 2154, 'Le': 2155, 'Monde': 2156, 'gloat': 2157, 'July': 2158, '12': 2159, 'raid': 2160, 'accuse': 2161, 'reactionary': 2162, 'Rhodesia': 2163, 'Africa': 2164, 'demonstration': 2165, 'military': 2166, 'superiority': 2167, 'western': 2168, 'upset': 2169, 'balance': 2170, 'rich': 2171, 'climate': 2172, 'treat': 2173, 'Third': 2174, 'partner': 2175, 'rhodesian': 2176, 'Africans': 2177, 'Israelis': 2178, 'champagne': 2179, 'european': 2180, 'approval': 2181, 'endanger': 2182, 'plan': 2183, 'France': 2184, 'international': 2185, 'rescue': 2186, 'wisecrack': 2187, 'Amin': 2188, 'speech': 2189, 'Port': 2190, 'OAS': 2191, 'provoke': 2192, 'applause': 2193, 'delegate': 2194, 'hostage': 2195, 'comfortable': 2196, 'explosive': 2197, 'weep': 2198, 'beg': 2199, 'everybody': 2200, 'David': 2201, 'Shahar': 2202, 'whose': 2203, 'chest': 2204, 'deep': 2205, 'breath': 2206, 'advise': 2207, 'nourishing': 2208, 'sage': 2209, 'delicacy': 2210, 'Dead': 2211, 'Sea': 2212, 'bulbous': 2213, 'roof': 2214, 'color': 2215, 'deadness': 2216, 'melting': 2217, 'human': 2218, 'weight': 2219, 'intelligible': 2220, 'metaphysical': 2221, 'universe': 2222, 'openness': 2223, 'rockjumbled': 2224, 'valley': 2225, 'elsewhere': 2226, 'disintegrate': 2227, 'mingle': 2228, 'Mishkenot': 2229, \"Sha'ananim\": 2230, 'slope': 2231, 'Mount': 2232, 'Zion': 2233, 'Old': 2234, 'City': 2235, 'Gai': 2236, 'Hinnom': 2237, 'Gehenna': 2238, 'tradition': 2239, 'worshiper': 2240, 'Moloch': 2241, 'ancient': 2242, 'Karaite': 2243, 'burial': 2244, 'mingling': 2245, 'yourself': 2246, 'queerly': 2247, 'nerve': 2248, 'dust': 2249, 'grind': 2250, 'geologically': 2251, 'dolomite': 2252, 'clay': 2253, 'hoarier': 2254, 'Gray': 2255, 'Bloom': 2256, 'Joyce': 2257, 'Ulysses': 2258, 'brilliant': 2259, 'massive': 2260, 'crumpled': 2261, 'mountain': 2262, 'exhaustion': 2263, 'atmosphere': 2264, 'american': 2265, 'commonplace': 2266, 'true': 2267, 'soul': 2268, 'municipality': 2269, 'Wolfson': 2270, 'Foundation': 2271, 'London': 2272, 'planting': 2273, 'garden': 2274, 'arab': 2275, 'kick': 2276, 'soccer': 2277, 'ball': 2278, 'East': 2279, 'toughie': 2280, 'stiffen': 2281, 'shoulder': 2282, 'practice': 2283, 'dangerous': 2284, 'loiterer': 2285, 'muscular': 2286, 'ornament': 2287, 'nag': 2288, 'horseshoe': 2289, 'bridle': 2290, 'writer': 2291, 'thoughtful': 2292, 'tout': 2293, 'tombe': 2294, 'cavern': 2295, 'niche': 2296, 'corpse': 2297, 'lay': 2298, 'fender': 2299, 'rust': 2300, 'century': 2301, 'metal': 2302, 'absolutely': 2303, 'Prophet': 2304, 'Jeremiah': 2305, 'Elie': 2306, 'Kedourie': 2307, 'Political': 2308, 'Memoirs': 2309, 'unknown': 2310, 'diplomacy': 2311, 'forty': 2312, 'Middle': 2313, 'United': 2314, 'States': 2315, 'rely': 2316, 'heavily': 2317, 'management': 2318, 'consultant': 2319, 'public': 2320, 'relation': 2321, 'expert': 2322, 'firm': 2323, 'Booz': 2324, 'Allen': 2325, 'amp': 2326, 'Hamilton': 2327, 'lend': 2328, 'specialist': 2329, 'Miles': 2330, 'Copeland': 2331, 'State': 2332, 'Department': 2333, '1955': 2334, 'member': 2335, 'Policy': 2336, 'Planning': 2337, 'Committee': 2338, 'main': 2339, 'friendship': 2340, 'ourselves': 2341, 'Nasser': 2342, '1947': 2343, 'Damascus': 2344, 'unofficial': 2345, 'syrian': 2346, 'probe': 2347, 'liberalize': 2348, 'political': 2349, 'democracy': 2350, 'Americans': 2351, 'fight': 2352, 'rig': 2353, 'election': 2354, 'Syria': 2355, 'corruption': 2356, 'despite': 2357, 'power': 2358, 'Frustrated': 2359, 'heavy': 2360, 'American': 2361, 'Minister': 2362, 'encourage': 2363, 'coup': 2364, \"d'etat\": 2365, 'bizarre': 2366, 'ambassador': 2367, 'minister': 2368, 'genuine': 2369, 'revolution': 2370, 'overthrow': 2371, 'landowner': 2372, 'crook': 2373, 'politician': 2374, 'elite': 2375, 'underpin': 2376, 'ruler': 2377, 'buttress': 2378, 'population': 2379, 'presumably': 2380, 'approve': 2381, 'legitimate': 2382, 'aim': 2383, 'whoever': 2384, 'equivalent': 2385, 'philosophical': 2386, 'Egypt': 2387, 'Kermit': 2388, 'Roosevelt': 2389, 'CIA': 2390, 'officer': 2391, 'involve': 2392, 'conspiracy': 2393, '22': 2394, '1952': 2395, 'regime': 2396, 'populace': 2397, 'literate': 2398, 'stable': 2399, 'class': 2400, 'sufficient': 2401, 'identification': 2402, 'local': 2403, 'ideal': 2404, 'truly': 2405, 'indigenous': 2406, 'democratic': 2407, 'institution': 2408, 'glide': 2409, 'realm': 2410, 'loan': 2411, 'egyptian': 2412, 'government': 2413, 'James': 2414, 'Eichelberger': 2415, 'scientist': 2416, 'account': 2417, 'executive': 2418, 'J': 2419, 'Walter': 2420, 'Thompson': 2421, 'advertising': 2422, 'Cairo': 2423, 'confidant': 2424, 'policy': 2425, 'Arabic': 2426, 'staff': 2427, 'Power': 2428, 'Problems': 2429, 'Revolutionary': 2430, 'Government': 2431, 'forth': 2432, 'accord': 2433, 'final': 2434, 'Zakaria': 2435, 'Mohieddin': 2436, 'reasonable': 2437, 'deputy': 2438, 'intelligence': 2439, 'analyst': 2440, 'C.I.A.': 2441, 'former': 2442, 'police': 2443, 'politisize': 2444, 'partisan': 2445, 'paramilitary': 2446, 'revolutionary': 2447, 'Leninism': 2448, 'neat': 2449, 'ice': 2450, 'bitter': 2451, 'safely': 2452, 'Jefferson': 2453, 'liberty': 2454, 'refresh': 2455, 'blood': 2456, 'patriot': 2457, 'tyrant': 2458, 'romantic': 2459, 'alive': 2460, 'prime': 2461, 'client': 2462, 'pure': 2463, 'professional': 2464, 'understanding': 2465, 'mass': 2466, 'cadre': 2467, 'shocking': 2468, 'suspicion': 2469, 'collaborator': 2470, 'seizure': 2471, 'solve': 2472, 'social': 2473, 'befriend': 2474, 'increase': 2475, 'strengthen': 2476, 'America': 2477, 'universal': 2478, 'equality': 2479, 'illusionless': 2480, 'tough': 2481, 'guy': 2482, 'scale': 2483, 'mover': 2484, 'shaker': 2485, 'shaper': 2486, 'destiny': 2487, 'surrender': 2488, 'fantasy': 2489, 'omnipotence': 2490, 'nation': 2491, 'plenipotentiary': 2492, 'confidently': 2493, 'Bolshevik': 2494, 'fire': 2495, 'doubt': 2496, 'resource': 2497, 'science': 2498, 'lesson': 2499, 'tyranny': 2500, 'puzzling': 2501, 'imparting': 2502, 'interest': 2503, 'contribute': 2504, 'welfare': 2505, 'intriguing': 2506, 'whence': 2507, 'passion': 2508, 'functionary': 2509, 'Fury': 2510, 'Compson': 2511, 'belong': 2512, 'e': 2513, 'E': 2514, 'Cummings': 2515, '1910': 2516, 'land': 2517, 'kike': 2518, 'wop': 2519, 'buy': 2520, 'italian': 2521, 'flinch': 2522, 'Chicago': 2523, 'Faulkner': 2524, 'guilty': 2525, 'offense': 2526, 'borrow': 2527, 'Ben': 2528, 'Gurion': 2529, 'illusion': 2530, 'goyish': 2531, 'emigrate': 2532, 'liberal': 2533, 'signify': 2534, 'govern': 2535, 'severe': 2536, 'leftist': 2537, 'critic': 2538, 'exception': 2539, 'Left': 2540, 'detractor': 2541, 'abuse': 2542, 'less': 2543, 'immigrant': 2544, 'North': 2545, 'Orient': 2546, 'denounce': 2547, 'corrupt': 2548, 'Levantine': 2549, 'theocratic': 2550, 'Gossip': 2551, 'trace': 2552, 'israeli': 2553, 'financial': 2554, 'swindle': 2555, 'observant': 2556, 'Orthodox': 2557, 'Ashkenazi': 2558, 'unimaginative': 2559, 'Rabin': 2560, 'lack': 2561, 'stature': 2562, 'terrible': 2563, 'generation': 2564, 'hostile': 2565, 'african': 2566, 'asian': 2567, 'oriental': 2568, 'blame': 2569, 'baksheesh': 2570, 'mentality': 2571, 'intellectual': 2572, 'quality': 2573, 'deplorable': 2574, 'phrase': 2575, 'deteriorate': 2576, 'society': 2577, 'immediately': 2578, 'paradox': 2579, 'Jakov': 2580, 'Lind': 2581, 'quote': 2582, 'hell': 2583, 'await': 2584, 'personal': 2585, 'dissatisfaction': 2586, 'mediocre': 2587, 'inordinate': 2588, 'demand': 2589, 'uncomfortable': 2590, 'Christianity': 2591, 'persistency': 2592, 'Jacques': 2593, 'Maritain': 2594, 'characterize': 2595, 'anti': 2596, 'Semitism': 2597, 'rid': 2598, 'moral': 2599, 'burden': 2600, 'disaster': 2601, 'history': 2602, 'zone': 2603, 'horrible': 2604, 'irony': 2605, 'conveniently': 2606, 'Holocaust': 2607, 'accompany': 2608, 'reflection': 2609, 'partly': 2610, 'proud': 2611, 'mostly': 2612, 'genius': 2613, 'heart': 2614, 'crisis': 2615, 'Valley': 2616, 'Jehosaphat': 2617, 'tomb': 2618, 'road': 2619, 'acre': 2620, 'cave': 2621, 'grave': 2622, 'litter': 2623, 'schoolroom': 2624, 'singe': 2625, 'November': 2626, 'uncomfortably': 2627, 'Jordanians': 2628, 'tear': 2629, 'jordanian': 2630, 'herodian': 2631, 'relic': 2632, 'distort': 2633, 'Absalom': 2634, 'funnel': 2635, 'tapering': 2636, 'army': 2637, 'direction': 2638, 'interminable': 2639, 'fine': 2640, 'obsess': 2641, 'lamentation': 2642, 'Messiah': 2643, 'trumpet': 2644, 'hen': 2645, 'scratch': 2646, 'peck': 2647, 'speckle': 2648, 'party': 2649, 'dungaree': 2650, 'sleeve': 2651, 'waist': 2652, 'muslim': 2653, 'cemetery': 2654, 'significant': 2655, 'historical': 2656, 'private': 2657, 'sly': 2658, 'submit': 2659, 'Stendhal': 2660, 'hero': 2661, 'prison': 2662, 'french': 2663, 'aesthetic': 2664, 'paradise': 2665, 'detention': 2666, 'Ferte': 2667, 'Mace': 2668, 'brave': 2669, 'modern': 2670, 'Mandelstams': 2671, 'Sinyavskys': 2672, 'hunger': 2673, 'Siberia': 2674, 'Osip': 2675, 'Mandelstam': 2676, 'recite': 2677, 'convict': 2678, 'request': 2679, 'Andrei': 2680, 'Sinyavsky': 2681, 'journal': 2682, 'politic': 2683, 'experience': 2684, 'recover': 2685, 'proper': 2686, 'foreground': 2687, 'John': 2688, 'Auerbach': 2689, 'Caesarea': 2690, 'kibbutznik': 2691, 'seaman': 2692, 'voyage': 2693, 'dear': 2694, 'warn': 2695, 'contrary': 2696, 'sixty': 2697, 'draw': 2698, 'slight': 2699, 'delicate': 2700, 'chief': 2701, 'ticket': 2702, 'complicate': 2703, 'emergency': 2704, 'repair': 2705, 'ocean': 2706, 'boyish': 2707, 'bearded': 2708, 'copper': 2709, 'nervous': 2710, 'valise': 2711, 'booze': 2712, 'pyjama': 2713, 'delighted': 2714, 'suffer': 2715, 'activate': 2716, 'grieve': 2717, 'Adam': 2718, 'warfare': 2719, 'unit': 2720, 'action': 2721, 'helicopter': 2722, 'embrace': 2723, 'sunny': 2724, 'numb': 2725, 'wasp': 2726, 'autumn': 2727, 'aquavit': 2728, 'passionately': 2729, 'discuss': 2730, 'literature': 2731, 'marvelous': 2732, 'suffering': 2733, 'hill': 2734, 'Moab': 2735, 'sixteen': 2736, 'Warsaw': 2737, 'ghetto': 2738, 'parent': 2739, 'sister': 2740, 'kill': 2741, 'everyone': 2742, 'obtain': 2743, 'polish': 2744, 'engine': 2745, 'german': 2746, 'freighter': 2747, 'war': 2748, 'via': 2749, 'Cyprus': 2750, 'join': 2751, 'Kibbutz': 2752, 'Sdot': 2753, 'Yam': 2754, 'married': 2755, 'cancer': 2756, 'typical': 2757, 'Eastern': 2758, 'Europe': 2759, 'completely': 2760, 'mother': 2761, 'Germans': 2762, 'harvesting': 2763, 'kibbutz': 2764, 'exceptional': 2765, 'sail': 2766, 'infrequently': 2767, 'huge': 2768, 'tanker': 2769, 'supermechanize': 2770, 'ultraefficient': 2771, 'crew': 2772, 'port': 2773, 'cargo': 2774, 'potash': 2775, 'steel': 2776, 'Naples': 2777, 'weather': 2778, 'harbor': 2779, 'anchor': 2780, 'japanese': 2781, 'ship': 2782, 'pilot': 2783, 'tug': 2784, 'official': 2785, 'claim': 2786, 'incapacitate': 2787, 'captain': 2788, 'post': 2789, 'bond': 2790, 'expense': 2791, 'crippled': 2792, 'berth': 2793, 'briefly': 2794, 'dispute': 2795, 'unloaded': 2796, 'demurrage': 2797, 'fee': 2798, 'mount': 2799, 'holdup': 2800, 'racketeer': 2801, 'con': 2802, 'going': 2803, 'slang': 2804, 'Haifa': 2805, 'protection': 2806, 'insurance': 2807, 'company': 2808, 'town': 2809, 'waiter': 2810, 'bartender': 2811, 'wipe': 2812, 'continually': 2813, 'community': 2814, 'jam': 2815, 'worthy': 2816, 'lane': 2817, 'screw': 2818, 'honking': 2819, 'madly': 2820, 'standstill': 2821, 'evening': 2822, 'snarl': 2823, 'nearby': 2824, 'beach': 2825, 'Mississippi': 2826, 'tourist': 2827, 'bathing': 2828, 'cabin': 2829, 'nail': 2830, 'lovely': 2831, 'pang': 2832, 'Sixth': 2833, 'Fleet': 2834, 'carrier': 2835, 'F': 2836, 'Kennedy': 2837, 'shore': 2838, 'civilian': 2839, 'clothing': 2840, 'rowdy': 2841, 'Oklahoma': 2842, 'Tulsa': 2843, 'especially': 2844, 'interested': 2845, 'delight': 2846, 'ignorance': 2847, 'refreshing': 2848, 'sailor': 2849, 'holocaust': 2850, 'tank': 2851, 'desert': 2852, 'terrorist': 2853, 'bomb': 2854, 'sea': 2855, 'shorthande': 2856, 'chat': 2857, 'confidante': 2858, 'silly': 2859, 'quarter': 2860, 'Stromboli': 2861, 'streak': 2862, 'crimson': 2863, 'lava': 2864, 'volcano': 2865, 'phenomenon': 2866, 'island': 2867, 'mast': 2868, 'Balkans': 2869, 'village': 2870, 'church': 2871, 'locker': 2872, 'bird': 2873, 'badly': 2874, 'dumb': 2875, 'bastard': 2876, 'foul': 2877, 'pump': 2878, 'ballast': 2879, 'pollute': 2880, 'Harold': 2881, 'Rosenberg': 2882, 'freely': 2883, 'plane': 2884, 'suck': 2885, 'gale': 2886, 'exposition': 2887, 'harangue': 2888, 'expostulation': 2889, 'threat': 2890, 'prophecy': 2891, 'diplomat': 2892, 'cagey': 2893, 'explanation': 2894, 'responsible': 2895, 'cautious': 2896, 'grudging': 2897, 'rephrasing': 2898, 'amend': 2899, 'deadly': 2900, 'division': 2901, 'passionate': 2902, 'denunciation': 2903, 'Western': 2904, 'Russia': 2905, 'utterly': 2906, 'attentive': 2907, 'shoreless': 2908, 'subject': 2909, 'ultimately': 2910, 'survival': 2911, 'decent': 2912, 'decade': 2913, 'grasp': 2914, 'antiquity': 2915, 'daily': 2916, 'utility': 2917, 'shop': 2918, 'supermarket': 2919, 'symphony': 2920, 'orchestra': 2921, 'radio': 2922, 'music': 2923, 'explosion': 2924, 'Jaffa': 2925, 'Road': 2926, 'wounded': 2927, 'Uneasy': 2928, 'explode': 2929, 'dynamite': 2930, 'End': 2931, 'restaurant': 2932, 'fundamental': 2933, 'England': 2934, 'charming': 2935, 'dining': 2936, '1973': 2937, 'coolly': 2938, 'sweet': 2939, 'flower': 2940, 'lamp': 2941, 'family': 2942, 'adolescent': 2943, 'school': 2944, 'domestic': 2945, 'ceremony': 2946, 'destructive': 2947, 'enemy': 2948, 'unchanged': 2949, 'creation': 2950, 'pleasantly': 2951, 'refuse': 2952, 'admit': 2953, 'historic': 2954, 'uneasiness': 2955, 'immovable': 2956, 'Nellie': 2957, 'cruising': 2958, 'yawl': 2959, 'swing': 2960, 'flutter': 2961, 'wind': 2962, 'river': 2963, 'tide': 2964, 'Thames': 2965, 'waterway': 2966, 'offing': 2967, 'weld': 2968, 'joint': 2969, 'luminous': 2970, 'tan': 2971, 'barge': 2972, 'cluster': 2973, 'canvas': 2974, 'sharply': 2975, 'peak': 2976, 'gleam': 2977, 'varnish': 2978, 'sprit': 2979, 'haze': 2980, 'flatness': 2981, 'Gravesend': 2982, 'condense': 2983, 'mournful': 2984, 'gloom': 2985, 'brood': 2986, 'motionless': 2987, 'earth': 2988, 'Director': 2989, 'Companies': 2990, 'host': 2991, 'affectionately': 2992, 'bow': 2993, 'seaward': 2994, 'nautical': 2995, 'resemble': 2996, 'trustworthiness': 2997, 'personify': 2998, 'estuary': 2999, 'brooding': 3000, 'besides': 3001, 'period': 3002, 'separation': 3003, 'tolerant': 3004, 'yarn': 3005, 'Lawyer': 3006, 'virtue': 3007, 'cushion': 3008, 'deck': 3009, 'rug': 3010, 'Accountant': 3011, 'domino': 3012, 'architecturally': 3013, 'Marlow': 3014, 'legged': 3015, 'mizzen': 3016, 'cheek': 3017, 'complexion': 3018, 'straight': 3019, 'ascetic': 3020, 'aspect': 3021, 'palm': 3022, 'outward': 3023, 'idol': 3024, 'satisfy': 3025, 'amongst': 3026, 'exchange': 3027, 'lazily': 3028, 'afterwards': 3029, 'silence': 3030, 'board': 3031, 'yacht': 3032, 'meditative': 3033, 'placid': 3034, 'staring': 3035, 'serenity': 3036, 'exquisite': 3037, 'brilliance': 3038, 'pacifically': 3039, 'benign': 3040, 'immensity': 3041, 'unstained': 3042, 'mist': 3043, 'Essex': 3044, 'marsh': 3045, 'gauzy': 3046, 'radiant': 3047, 'fabric': 3048, 'woode': 3049, 'inland': 3050, 'drape': 3051, 'diaphanous': 3052, 'fold': 3053, 'upper': 3054, 'somber': 3055, 'anger': 3056, 'approach': 3057, 'curved': 3058, 'imperceptible': 3059, 'glow': 3060, 'dull': 3061, 'ray': 3062, 'heat': 3063, 'stricken': 3064, 'crowd': 3065, 'forthwith': 3066, 'profound': 3067, 'unruffle': 3068, 'decline': 3069, 'race': 3070, 'bank': 3071, 'dignity': 3072, 'uttermost': 3073, 'venerable': 3074, 'stream': 3075, 'vivid': 3076, 'flush': 3077, 'depart': 3078, 'august': 3079, 'abide': 3080, 'reverence': 3081, 'affection': 3082, 'evoke': 3083, 'spirit': 3084, 'tidal': 3085, 'fro': 3086, 'unceasing': 3087, 'battle': 3088, 'Sir': 3089, 'Francis': 3090, 'Drake': 3091, 'Franklin': 3092, 'untitled': 3093, 'errant': 3094, 'jewel': 3095, 'flash': 3096, 'Golden': 3097, 'Hind': 3098, 'round': 3099, 'flank': 3100, 'treasure': 3101, 'Queen': 3102, 'Highness': 3103, 'gigantic': 3104, 'tale': 3105, 'Erebus': 3106, 'Terror': 3107, 'conquest': 3108, 'Deptford': 3109, 'Greenwich': 3110, 'Erith': 3111, 'adventurer': 3112, 'settler': 3113, 'king': 3114, 'Change': 3115, 'admiral': 3116, 'interloper': 3117, 'eastern': 3118, 'trade': 3119, 'commission': 3120, 'India': 3121, 'fleet': 3122, 'hunter': 3123, 'gold': 3124, 'pursuer': 3125, 'fame': 3126, 'sword': 3127, 'torch': 3128, 'messenger': 3129, 'bearer': 3130, 'sacred': 3131, 'greatness': 3132, 'float': 3133, 'ebb': 3134, '...': 3135, 'seed': 3136, 'commonwealth': 3137, 'germ': 3138, 'empire': 3139, 'dusk': 3140, 'Chapman': 3141, 'lighthouse': 3142, 'erect': 3143, 'mud': 3144, 'flat': 3145, 'strongly': 3146, 'fairway': 3147, 'stir': 3148, 'monstrous': 3149, 'ominously': 3150, 'sunshine': 3151, 'lurid': 3152, 'glare': 3153, 'star': 3154, 'represent': 3155, 'wanderer': 3156, 'express': 3157, 'sedentary': 3158, 'immutability': 3159, 'surrounding': 3160, 'veil': 3161, 'sense': 3162, 'slightly': 3163, 'disdainful': 3164, 'mysterious': 3165, 'unless': 3166, 'mistress': 3167, 'inscrutable': 3168, 'Destiny': 3169, 'casual': 3170, 'stroll': 3171, 'spree': 3172, 'suffice': 3173, 'unfold': 3174, 'continent': 3175, 'generally': 3176, 'worth': 3177, 'direct': 3178, 'simplicity': 3179, 'shell': 3180, 'nut': 3181, 'propensity': 3182, 'episode': 3183, 'kernel': 3184, 'envelop': 3185, 'likeness': 3186, 'misty': 3187, 'halo': 3188, 'spectral': 3189, 'illumination': 3190, 'moonshine': 3191, 'remark': 3192, 'surprising': 3193, 'grunt': 3194, 'presently': 3195, 'Romans': 3196, 'nineteen': 3197, '....': 3198, 'Light': 3199, 'Knights': 3200, 'blaze': 3201, 'plain': 3202, 'lightning': 3203, 'yesterday': 3204, 'commander': 3205, 'ye': 3206, 'them': 3207, 'trireme': 3208, 'Mediterranean': 3209, 'overland': 3210, 'Gauls': 3211, 'hurry': 3212, 'craft': 3213, 'legionary': 3214, 'wonderful': 3215, 'lot': 3216, 'handy': 3217, 'apparently': 3218, 'concertina': 3219, 'sandbank': 3220, 'forest': 3221, 'savage': 3222, 'precious': 3223, 'falernian': 3224, 'wine': 3225, 'ashore': 3226, 'wilderness': 3227, 'needle': 3228, 'bundle': 3229, 'hay': 3230, 'fog': 3231, 'tempest': 3232, 'disease': 3233, 'exile': 3234, 'skulking': 3235, 'oh': 3236, 'brag': 3237, 'cheer': 3238, 'promotion': 3239, 'Ravenna': 3240, 'survive': 3241, 'citizen': 3242, 'toga': 3243, 'dice': 3244, 'train': 3245, 'prefect': 3246, 'tax': 3247, 'gatherer': 3248, 'trader': 3249, 'mend': 3250, 'fortune': 3251, 'swamp': 3252, 'march': 3253, 'wood': 3254, 'savagery': 3255, 'utter': 3256, 'jungle': 3257, 'wild': 3258, 'initiation': 3259, 'midst': 3260, 'incomprehensible': 3261, 'detestable': 3262, 'fascination': 3263, 'abomination': 3264, 'regret': 3265, 'longing': 3266, 'powerless': 3267, 'disgust': 3268, 'hate': 3269, 'Mind': 3270, 'lift': 3271, 'elbow': 3272, 'pose': 3273, 'Buddha': 3274, 'preaching': 3275, 'lotus': 3276, 'none': 3277, 'efficiency': 3278, 'devotion': 3279, 'chap': 3280, 'colonist': 3281, 'administration': 3282, 'squeeze': 3283, 'conqueror': 3284, 'brute': 3285, 'boast': 3286, 'strength': 3287, 'accident': 3288, 'arise': 3289, 'weakness': 3290, 'grab': 3291, 'sake': 3292, 'robbery': 3293, 'violence': 3294, 'aggravate': 3295, 'murder': 3296, 'tackle': 3297, 'flatter': 3298, 'nose': 3299, 'pretty': 3300, 'redeem': 3301, 'sentimental': 3302, 'pretense': 3303, 'unselfish': 3304, 'offer': 3305, 'flame': 3306, 'pursue': 3307, 'overtake': 3308, 'separate': 3309, 'hastily': 3310, 'deepening': 3311, 'sleepless': 3312, 'patiently': 3313, 'till': 3314, 'hesitating': 3315, 'suppose': 3316, 'fresh': 3317, 'fate': 3318, 'inconclusive': 3319, 'personally': 3320, 'teller': 3321, 'unaware': 3322, 'audience': 3323, 'ought': 3324, 'farth': 3325, 'culminating': 3326, 'pitiful': 3327, 'extraordinary': 3328, 'Indian': 3329, 'Ocean': 3330, 'Pacific': 3331, 'China': 3332, 'Seas': 3333, 'regular': 3334, 'dose': 3335, 'loaf': 3336, 'hinder': 3337, 'invade': 3338, 'heavenly': 3339, 'mission': 3340, 'civilize': 3341, 'tired': 3342, 'Australia': 3343, 'glory': 3344, 'exploration': 3345, 'invite': 3346, 'Pole': 3347, 'shall': 3348, 'glamour': 3349, 'Equator': 3350, 'latitude': 3351, 'hemisphere': 3352, 'hankering': 3353, 'got': 3354, 'boyhood': 3355, 'lake': 3356, 'cease': 3357, 'delightful': 3358, 'patch': 3359, 'gloriously': 3360, 'mighty': 3361, 'immense': 3362, 'snake': 3363, 'uncoiled': 3364, 'curve': 3365, 'afar': 3366, 'vast': 3367, 'tail': 3368, 'depth': 3369, 'fascinate': 3370, 'concern': 3371, 'dash': 3372, 'steamboat': 3373, 'charm': 3374, 'Continental': 3375, 'trading': 3376, 'cheap': 3377, 'nasty': 3378, 'venture': 3379, 'Company': 3380, 'profit': 3381, 'Charlie': 3382, 'laborer': 3383, 'queer': 3384, 'truth': 3385, 'sunset': 3386, 'confound': 3387, 'contentedly': 3388, 'knock': 3389, 'flannel': 3390, 'impostor': 3391, 'crossing': 3392, 'hesitation': 3393, 'startled': 3394, 'steamer': 3395, 'sole': 3396, 'soldier': 3397, 'coast': 3398, 'slip': 3399, 'enigma': 3400, 'frown': 3401, 'inviting': 3402, 'grand': 3403, 'insipid': 3404, 'mute': 3405, 'whispering': 3406, 'featureless': 3407, 'making': 3408, 'monotonous': 3409, 'grimness': 3410, 'edge': 3411, 'colossal': 3412, 'surf': 3413, 'glitter': 3414, 'creep': 3415, 'fierce': 3416, 'glisten': 3417, 'drip': 3418, 'steam': 3419, 'grayish': 3420, 'whitish': 3421, 'flag': 3422, 'settlement': 3423, 'pin': 3424, 'untouched': 3425, 'expanse': 3426, 'background': 3427, 'pound': 3428, 'clerk': 3429, 'levy': 3430, 'toll': 3431, 'forsake': 3432, 'tin': 3433, 'shed': 3434, 'pole': 3435, 'nobody': 3436, 'fling': 3437, 'Gran': 3438, 'Bassam': 3439, 'Little': 3440, 'Popo': 3441, 'sordid': 3442, 'farce': 3443, 'sinister': 3444, 'backcloth': 3445, 'idleness': 3446, 'passenger': 3447, 'isolation': 3448, 'oily': 3449, 'languid': 3450, 'somberness': 3451, 'toil': 3452, 'senseless': 3453, 'positive': 3454, 'brother': 3455, 'boat': 3456, 'momentary': 3457, 'reality': 3458, 'paddle': 3459, 'eyeball': 3460, 'sing': 3461, 'perspiration': 3462, 'grotesque': 3463, 'mask': 3464, 'muscle': 3465, 'vitality': 3466, 'intense': 3467, 'straightforward': 3468, 'scare': 3469, 'French': 3470, 'thereabout': 3471, 'ensign': 3472, 'limp': 3473, 'rag': 3474, 'muzzle': 3475, 'inch': 3476, 'gun': 3477, 'hull': 3478, 'greasy': 3479, 'slimy': 3480, 'swell': 3481, 'sway': 3482, 'pop': 3483, 'dart': 3484, 'projectile': 3485, 'feeble': 3486, 'screech': 3487, 'insanity': 3488, 'proceeding': 3489, 'lugubrious': 3490, 'drollery': 3491, 'dissipate': 3492, 'somebody': 3493, 'assure': 3494, 'earnestly': 3495, 'native': 3496, 'lonely': 3497, 'fever': 3498, 'farcical': 3499, 'dance': 3500, 'earthy': 3501, 'overheated': 3502, 'catacomb': 3503, 'formless': 3504, 'Nature': 3505, 'herself': 3506, 'ward': 3507, 'intruder': 3508, 'rot': 3509, 'thicken': 3510, 'slime': 3511, 'contorted': 3512, 'mangrove': 3513, 'writhe': 3514, 'extremity': 3515, 'impotent': 3516, 'particularized': 3517, 'vague': 3518, 'oppressive': 3519, 'weary': 3520, 'pilgrimage': 3521, 'mile': 3522, 'higher': 3523, 'passage': 3524, 'swede': 3525, 'bridge': 3526, 'morose': 3527, 'lanky': 3528, 'gait': 3529, 'miserable': 3530, 'wharf': 3531, 'contemptuously': 3532, 'bitterness': 3533, 'funny': 3534, 'franc': 3535, 'o': 3536, 'athwart': 3537, 'ahead': 3538, 'vigilantly': 3539, 'watchfully': 3540, 'rocky': 3541, 'cliff': 3542, 'mound': 3543, 'iron': 3544, 'excavation': 3545, 'declivity': 3546, 'continuous': 3547, 'noise': 3548, 'inhabited': 3549, 'devastation': 3550, 'naked': 3551, 'ant': 3552, 'jetty': 3553, 'sudden': 3554, 'recrudescence': 3555, 'station': 3556, 'barrack': 3557, 'farewell': 3558, 'boiler': 3559, 'wallowing': 3560, 'aside': 3561, 'bowlder': 3562, 'undersized': 3563, 'railway': 3564, 'wheel': 3565, 'carcass': 3566, 'animal': 3567, 'decay': 3568, 'stack': 3569, 'rusty': 3570, 'rail': 3571, 'shady': 3572, 'feebly': 3573, 'blink': 3574, 'steep': 3575, 'horn': 3576, 'toot': 3577, 'detonation': 3578, 'puff': 3579, 'objectless': 3580, 'blasting': 3581, 'clink': 3582, 'basket': 3583, 'footstep': 3584, 'wound': 3585, 'loin': 3586, 'wag': 3587, 'rib': 3588, 'limb': 3589, 'knot': 3590, 'rope': 3591, 'collar': 3592, 'chain': 3593, 'bight': 3594, 'rhythmically': 3595, 'clinking': 3596, 'ominous': 3597, 'imagination': 3598, 'criminal': 3599, 'outraged': 3600, 'law': 3601, 'bursting': 3602, 'insoluble': 3603, 'meager': 3604, 'violently': 3605, 'dilate': 3606, 'nostril': 3607, 'quiver': 3608, 'stonily': 3609, 'uphill': 3610, 'deathlike': 3611, 'indifference': 3612, 'unhappy': 3613, 'raw': 3614, 'reclaimed': 3615, 'despondently': 3616, 'rifle': 3617, 'hoist': 3618, 'weapon': 3619, 'alacrity': 3620, 'simple': 3621, 'prudence': 3622, 'alike': 3623, 'speedily': 3624, 'reassure': 3625, 'rascally': 3626, 'grin': 3627, 'partnership': 3628, 'exalted': 3629, 'trust': 3630, 'gang': 3631, 'tender': 3632, 'fend': 3633, 'resist': 3634, 'attack': 3635, 'cost': 3636, 'blunder': 3637, 'devil': 3638, 'greed': 3639, 'desire': 3640, 'strong': 3641, 'lusty': 3642, 'eyed': 3643, 'drive': 3644, 'hillside': 3645, 'foresaw': 3646, 'blinding': 3647, 'acquaint': 3648, 'flabby': 3649, 'weak': 3650, 'rapacious': 3651, 'pitiless': 3652, 'folly': 3653, 'insidious': 3654, 'appalled': 3655, 'warning': 3656, 'obliquely': 3657, 'artificial': 3658, 'hole': 3659, 'dig': 3660, 'divine': 3661, 'quarry': 3662, 'sandpit': 3663, 'anyhow': 3664, 'philanthropic': 3665, 'ravine': 3666, 'scar': 3667, 'drainage': 3668, 'tumble': 3669, 'wanton': 3670, 'smash': 3671, 'gloomy': 3672, 'circle': 3673, 'Inferno': 3674, 'uninterrupted': 3675, 'headlong': 3676, 'stillness': 3677, 'grove': 3678, 'leaf': 3679, 'launch': 3680, 'audible': 3681, 'crouch': 3682, 'trunk': 3683, 'cling': 3684, 'efface': 3685, 'dim': 3686, 'attitude': 3687, 'pain': 3688, 'abandonment': 3689, 'significance': 3690, 'wreck': 3691, 'fancy': 3692, 'stupid': 3693, 'nuisance': 3694, 'manager': 3695, 'volunteer': 3696, 'skipper': 3697, 'plenty': 3698, 'fish': 3699, 'interview': 3700, 'size': 3701, 'ordinary': 3702, 'usual': 3703, 'trenchant': 3704, 'axe': 3705, 'disclaim': 3706, 'otherwise': 3707, 'indefinable': 3708, 'lip': 3709, 'stealthy': 3710, 'unconscious': 3711, 'intensify': 3712, 'instant': 3713, 'seal': 3714, 'commonest': 3715, 'youth': 3716, 'employ': 3717, 'obey': 3718, 'inspire': 3719, 'fear': 3720, 'Uneasiness': 3721, 'definite': 3722, 'mistrust': 3723, 'effective': 3724, 'faculty': 3725, 'organizing': 3726, 'initiative': 3727, 'evident': 3728, 'ill': 3729, 'term': 3730, 'triumphant': 3731, 'health': 3732, 'rout': 3733, 'constitution': 3734, 'riot': 3735, 'pompously': 3736, 'Jack': 3737, 'gather': 3738, 'originate': 3739, 'tropical': 3740, 'agent': 3741, 'entrail': 3742, 'utterance': 3743, 'keeping': 3744, 'annoy': 3745, 'quarrel': 3746, 'precedence': 3747, 'mess': 3748, 'unalterable': 3749, 'civil': 3750, 'uncivil': 3751, 'quiet': 3752, 'overfe': 3753, 'negro': 3754, 'insolence': 3755, 'relieve': 3756, 'delay': 3757, 'sealing': 3758, 'wax': 3759, 'rumor': 3760, 'jeopardy': 3761, 'Mr.': 3762, 'Kurtz': 3763, 'irritable': 3764, 'Hang': 3765, 'ah': 3766, 'murmur': 3767, 'anxiety': 3768, 'uneasy': 3769, 'fidget': 3770, 'dumbfound': 3771, 'futile': 3772, 'hut': 3773, 'veranda': 3774, 'idiot': 3775, 'startlingly': 3776, 'nicety': 3777, 'estimate': 3778, 'requisite': 3779, 'turning': 3780, 'yard': 3781, 'stave': 3782, 'faithless': 3783, 'pilgrim': 3784, 'bewitch': 3785, 'rotten': 3786, 'fence': 3787, 'ivory': 3788, 'rang': 3789, 'whisper': 3790, 'sigh': 3791, 'taint': 3792, 'imbecile': 3793, 'rapacity': 3794, 'whiff': 3795, 'Jove': 3796, 'unreal': 3797, 'invincible': 3798, 'evil': 3799, 'passing': 3800, 'fantastic': 3801, 'invasion': 3802, 'cotton': 3803, 'bead': 3804, 'avenging': 3805, 'consume': 3806, 'trash': 3807, 'quietly': 3808, 'dismantled': 3809, 'caper': 3810, 'stout': 3811, 'mustache': 3812, 'pail': 3813, 'behave': 3814, 'splendidly': 3815, 'dip': 3816, 'quart': 3817, 'hopeless': 3818, 'heap': 3819, 'ember': 3820, 'fiercely': 3821, 'nigger': 3822, 'beat': 3823, 'horribly': 3824, 'sick': 3825, 'pronounce': 3826, 'unfortunate': 3827, 'eh': 3828, 'incredible': 3829, 'gentlemanly': 3830, 'fork': 3831, 'hooked': 3832, 'offish': 3833, 'spy': 3834, 'hiss': 3835, 'perceive': 3836, 'aristocrat': 3837, 'dressing': 3838, 'candle': 3839, 'mat': 3840, 'collection': 3841, 'spear': 3842, 'assegais': 3843, 'shield': 3844, 'knife': 3845, 'trophy': 3846, 'business': 3847, 'intrust': 3848, 'inform': 3849, 'fragment': 3850, 'straw': 3851, 'maybe': 3852, 'anyways': 3853, 'likely': 3854, 'uncongenial': 3855, 'occupation': 3856, 'beguile': 3857, 'backbite': 3858, 'foolish': 3859, 'appoint': 3860, 'slander': 3861, 'effectually': 3862, 'heaven': 3863, 'horse': 3864, 'halter': 3865, 'charitable': 3866, 'saint': 3867, 'standpoint': 3868, 'evaluation': 3869, 'implementation': 3870, 'budget': 3871, 'dimension': 3872, 'override': 3873, 'legality': 3874, 'regularity': 3875, 'vote': 3876, 'Council': 3877, 'extremely': 3878, 'concerned': 3879, 'serious': 3880, 'decision': 3881, 'regard': 3882, 'Ojala': 3883, 'recommendation': 3884, 'reading': 3885, 'A4': 3886, '0072/97': 3887, 'Conservatives': 3888, 'Commission': 3889, 'forward': 3890, 'proposal': 3891, 'European': 3892, 'Single': 3893, 'Market': 3894, 'Union': 3895, 'resale': 3896, 'legislation': 3897, 'eleven': 3898, 'Member': 3899, 'maintain': 3900, 'preparatory': 3901, 'economic': 3902, 'aid': 3903, 'Albania': 3904, 'Greece': 3905, 'Italy': 3906, 'Spain': 3907, 'Austria': 3908, 'transition': 3909, 'successfully': 3910, 'continuity': 3911, 'Kong': 3912, 'Hong': 3913, 'rooted': 3914, 'House': 3915, 'firmly': 3916, 'unsuccessful': 3917, 'equally': 3918, 'damage': 3919, 'disagreement': 3920, 'technical': 3921, 'today': 3922, 'refiner': 3923, 'refinery': 3924, 'diesel': 3925, 'Japan': 3926, 'precisely': 3927, 'friendly': 3928, 'fuel': 3929, 'issue': 3930, 'employment': 3931, 'enlargement': 3932, 'politically': 3933, 'Membership': 3934, 'million': 3935, 'ecu': 3936, 'promote': 3937, 'confidence': 3938, 'naturally': 3939, 'turkish': 3940, 'cypriot': 3941, 'representation': 3942, 'lawful': 3943, 'internationally': 3944, 'no-6': 3945, 'Pirker': 3946, 'H-0218/97': 3947, 'Subject': 3948, 'Europol': 3949, 'major': 3950, 'obstacle': 3951, 'ratification': 3952, 'transitional': 3953, 'accession': 3954, 'guarantee': 3955, 'membership': 3956, 'negotiation': 3957, 'Portugal': 3958, 'Lomas': 3959, 'widely': 3960, 'servant': 3961, 'national': 3962, 'Britain': 3963, 'electioneering': 3964, 'Commissioners': 3965, 'boot': 3966, 'adopt': 3967, 'directive': 3968, '96/23': 3969, 'supervisory': 3970, 'EU': 3971, 'association': 3972, 'independent': 3973, 'interpreting': 3974, 'correctly': 3975, 'SjÃ¶stedt': 3976, 'No-15': 3977, 'Medina': 3978, 'Ortega': 3979, 'H-0237/97': 3980, 'industry': 3981, 'prosperity': 3982, 'research': 3983, 'conclusion': 3984, 'STOA': 3985, 'indicate': 3986, 'sulphur': 3987, 'secondly': 3988, 'refurbish': 3989, 'invest': 3990, 'continuously': 3991, 'rapporteur': 3992, '50': 3993, '30': 3994, 'ppm': 3995, 'excessive': 3996, 'appreciable': 3997, 'fiscal': 3998, 'vehicle': 3999, 'introduce': 4000, 'emerge': 4001, 'hearing': 4002, 'petrol': 4003, 'nine': 4004, 'Madam': 4005, 'President': 4006, 'improve': 4007, 'confrontation': 4008, 'concept': 4009, 'actual': 4010, 'driving': 4011, 'programme': 4012, 'innovation': 4013, 'April': 4014, 'September': 4015, 'concession': 4016, 'speaker': 4017, '2005': 4018, 'Amendments': 4019, 'Nos': 4020, '23': 4021, '37': 4022, '38': 4023, 'propose': 4024, 'clearly': 4025, 'misunderstanding': 4026, 'judge': 4027, 'frequently': 4028, 'indirect': 4029, 'discrimination': 4030, 'codify': 4031, 'Court': 4032, 'Justice': 4033, 'consistently': 4034, 'entitle': 4035, 'treatment': 4036, 'colleague': 4037, 'employer': 4038, 'mep': 4039, 'immediate': 4040, 'monitoring': 4041, 'absence': 4042, 'transparency': 4043, 'emphasize': 4044, 'responsibility': 4045, 'Mrs': 4046, 'Oomen': 4047, 'Ruijten': 4048, 'Glase': 4049, 'Minutes': 4050, 'session': 4051, 'Thursday': 4052, '29': 4053, 'May': 4054, '1997': 4055, 'raise': 4056, 'Green': 4057, 'agriculture': 4058, 'bee': 4059, 'petition': 4060, '1996': 4061, 'sadly': 4062, 'plaudit': 4063, 'Petitions': 4064, 'fully': 4065, 'win': 4066, 'Greek': 4067, 'owe': 4068, 'Commissioner': 4069, 'audio': 4070, 'boundary': 4071, 'inspector': 4072, 'Morse': 4073, 'Derrick': 4074, 'remedy': 4075, 'parliament': 4076, 'copyright': 4077, 'principle': 4078, 'competition': 4079, 'televising': 4080, 'sport': 4081, 'broadcaster': 4082, 'consideration': 4083, 'increasingly': 4084, 'noisy': 4085, 'complaint': 4086, 'competence': 4087, 'environmental': 4088, 'airport': 4089, 'motorway': 4090, 'refusal': 4091, 'encouragement': 4092, 'impunity': 4093, 'immunity': 4094, 'resignation': 4095, 'Vice': 4096, 'Cornelissen': 4097, 'agenda': 4098, 'Tuesday': 4099, 'oral': 4100, 'congratulation': 4101, 'non': 4102, 'practise': 4103, 'activity': 4104, 'doctor': 4105, 'teacher': 4106, 'Bonino': 4107, 'wholeheartedly': 4108, 'behalf': 4109, 'Kabul': 4110, 'Barbaric': 4111, 'closed': 4112, 'isolated': 4113, 'amendment': 4114, 'oppose': 4115, 'subsequently': 4116, 'reject': 4117, 'postpone': 4118, 'indefinitely': 4119, 'provision': 4120, 'restrict': 4121, 'physical': 4122, 'everyday': 4123, 'bemused': 4124, 'pragmatic': 4125, 'capital': 4126, 'income': 4127, 'compromise': 4128, 'finance': 4129, 'coordinate': 4130, 'objective': 4131, 'manoeuvring': 4132, 'distortion': 4133, 'market': 4134, 'simplification': 4135, 'accessory': 4136, 'sitting': 4137, 'suspend': 4138, '11.25': 4139, 'a.m.': 4140, '11.30': 4141, 'a.m': 4142, '..': 4143, 'predecessor': 4144, 'finish': 4145, 'barely': 4146, 'Macao': 4147, 'resolution': 4148, 'vital': 4149, 'influence': 4150, 'economy': 4151, 'prospect': 4152, 'Directive': 4153, '96/96': 4154, '/': 4155, 'EC': 4156, 'undergo': 4157, 'annual': 4158, 'roadworthiness': 4159, 'testing': 4160, 'centre': 4161, 'introduction': 4162, 'enthusiastically': 4163, 'solution': 4164, 'blindly': 4165, 'Community': 4166, 'intervention': 4167, 'exclusive': 4168, 'communication': 4169, 'consistency': 4170, 'regional': 4171, 'entrench': 4172, 'combat': 4173, 'unemployment': 4174, 'hearten': 4175, 'recognise': 4176, 'willing': 4177, 'dire': 4178, 'committee': 4179, 'exempt': 4180, 'intensive': 4181, 'disadvantage': 4182, 'exemption': 4183, 'kerosene': 4184, 'abolish': 4185, 'indexing': 4186, 'minimum': 4187, 'Cox': 4188, 'regulation': 4189, 'reliable': 4190, 'complex': 4191, 'criticism': 4192, 'Legal': 4193, 'Affairs': 4194, 'sided': 4195, 'salute': 4196, 'regulate': 4197, 'digital': 4198, 'safeguard': 4199, 'rightholder': 4200, 'Socialist': 4201, 'Group': 4202, 'limitation': 4203, 'context': 4204, 'logically': 4205, 'compensation': 4206, 'creator': 4207, 'sharing': 4208, 'cultural': 4209, 'mandatory': 4210, 'artwork': 4211, 'being': 4212, 'noticeable': 4213, 'balanced': 4214, 'achieve': 4215, 'incur': 4216, 'Barzanti': 4217, 'frankly': 4218, 'Blank': 4219, 'screen': 4220, 'enrich': 4221, 'La': 4222, 'BohÃ©me': 4223, 'garret': 4224, 'argue': 4225, 'Kingdom': 4226, 'tape': 4227, 'holder': 4228, 'absolute': 4229, 'banning': 4230, 'agricultural': 4231, 'trans': 4232, 'transport': 4233, 'network': 4234, 'priority': 4235, 'knell': 4236, 'recital': 4237, 'court': 4238, 'hit': 4239, 'producer': 4240, 'stranglehold': 4241, 'evolve': 4242, 'global': 4243, 'Parliament': 4244, 'uphold': 4245, 'remuneration': 4246, 'distinction': 4247, 'equity': 4248, 'measured': 4249, 'forwards': 4250, 'legal': 4251, 'sector': 4252, 'formula': 4253, 'harmonisation': 4254, 'C4': 4255, '0497/98': 4256, '98/0126': 4257, 'CNS': 4258, 'COM': 4259, 'industrial': 4260, 'sincere': 4261, 'Martin': 4262, 'Fischler': 4263, 'Agriculture': 4264, 'framework': 4265, 'overproduction': 4266, 'comprehensive': 4267, 'quantity': 4268, 'preserve': 4269, 'Article': 4270, '43': 4271, 'Van': 4272, 'Miert': 4273, 'progress': 4274, 'internal': 4275, 'medicinal': 4276, 'consumer': 4277, 'constitute': 4278, 'electricity': 4279, 'anxious': 4280, 'congratulate': 4281, 'Thors': 4282, 'Swedish': 4283, 'Finnish': 4284, 'habit': 4285, 'transfer': 4286, 'February': 4287, 'detailed': 4288, 'release': 4289, 'especial': 4290, 'No-46': 4291, 'Christine': 4292, 'Oddy': 4293, 'h-0002/99': 4294, 'pleased': 4295, 'honourable': 4296, 'Hatzidakis': 4297, 'van': 4298, 'den': 4299, 'Broek': 4300, 'No-59': 4301, 'Alex': 4302, 'Smith': 4303, 'H-0045/99': 4304, 'confess': 4305, 'totally': 4306, 'sensitivity': 4307, 'supplementary': 4308, 'protest': 4309, 'namely': 4310, 'closing': 4311, 'Sellafield': 4312, 'fitzsimon': 4313, 'invitation': 4314, 'tunnel': 4315, '7.15': 4316, 'p.m': 4317, 'resume': 4318, '9': 4319, 'p.m.': 4320, 'organisation': 4321, 'continuation': 4322, 'dynamic': 4323, 'equilibrium': 4324, 'draft': 4325, 'cap': 4326, 'reform': 4327, 'embed': 4328, 'rut': 4329, 'Rural': 4330, 'Development': 4331, 'growing': 4332, 'wholly': 4333, 'derive': 4334, 'vine': 4335, 'unfortunately': 4336, 'enormous': 4337, 'lady': 4338, 'gentleman': 4339, 'favour': 4340, 'commissioner': 4341, 'Brittan': 4342, 'forum': 4343, 'Trade': 4344, 'Organisation': 4345, 'urge': 4346, 'presidency': 4347, 'Bonn': 4348, 'US': 4349, 'summit': 4350, 'wallet': 4351, 'negotiate': 4352, 'condemn': 4353, 'pirate': 4354, 'Structural': 4355, 'Funds': 4356, 'financing': 4357, 'radical': 4358, 'Santer': 4359, 'rightly': 4360, 'pillar': 4361, 'Agenda': 4362, 'discussion': 4363, 'wane': 4364, 'headache': 4365, 'fraud': 4366, 'circumvention': 4367, 'evasion': 4368, 'compliment': 4369, 'Haug': 4370, 'appropriateness': 4371, 'unanimity': 4372, 'Cresson': 4373, 'reply': 4374, 'Valverde': 4375, 'premise': 4376, 'educate': 4377, 'guess': 4378, 'representative': 4379, 'Nicaragua': 4380, 'indicative': 4381, 'crystal': 4382, 'Rio': 4383, 'Coco': 4384, 'dialogue': 4385, 'Rules': 4386, 'originally': 4387, 'grateful': 4388, 'opening': 4389, 'damn': 4390, 'Kinnock': 4391, 'No-44': 4392, 'Bernie': 4393, 'Malone': 4394, 'H-0209/99': 4395, 'rectify': 4396, '7': 4397, '-%': 4398, 'sustainable': 4399, 'fishery': 4400, 'overall': 4401, '6500': 4402, 'tonne': 4403, 'trend': 4404, 'consumption': 4405, 'trafficking': 4406, 'depredation': 4407, 'smuggler': 4408, 'Morocco': 4409, 'grim': 4410, 'EDU': 4411, 'mandate': 4412, 'smuggling': 4413, 'ply': 4414, 'Gibraltar': 4415, 'authority': 4416, 'cooperate': 4417, 'route': 4418, 'No-49': 4419, 'written': 4420, 'monitor': 4421, 'origin': 4422, 'individual': 4423, 'statistic': 4424, 'Cuba': 4425, 'scottish': 4426, '84': 4427, 'exporter': 4428, 'undertaking': 4429, 'adoption': 4430, 'Burma': 4431, 'withdrawal': 4432, 'GSP': 4433, 'meat': 4434, 'modest': 4435, 'reimburse': 4436, 'bill': 4437, '60': 4438, '111': 4439, 'Time': 4440, 'electoral': 4441, 'masse': 4442, 'stage': 4443, 'Labour': 4444, 'dissent': 4445, 'Germany': 4446, 'surplus': 4447, 'billion': 4448, 'firstly': 4449, 'receipt': 4450, 'requirement': 4451, 'sincerely': 4452, 'purely': 4453, 'linguistic': 4454, 'conciliation': 4455, 'declare': 4456, 'seaport': 4457, 'famous': 4458, 'no-8': 4459, '14': 4460, 'endorse': 4461, 'Essen': 4462, 'summarise': 4463, 'gain': 4464, 'accurate': 4465, 'mediterranean': 4466, 'viability': 4467, 'infrastructure': 4468, 'investment': 4469, 'competitive': 4470, 'revenue': 4471, 'No-6': 4472, 'guideline': 4473, 'prolonged': 4474, 'privatisation': 4475, 'liberalisation': 4476, 'cooperation': 4477, 'jurisdiction': 4478, 'consequently': 4479, 'intend': 4480, 'worried': 4481, 'opportunity': 4482, 'renaissance': 4483, 'strive': 4484, 'authorised': 4485, 'applicant': 4486, 'capacity': 4487, 'recent': 4488, 'effort': 4489, 'secure': 4490, 'unnecessary': 4491, 'sympathy': 4492, 'prescriptive': 4493, 'reliance': 4494, 'commitment': 4495, 'constructive': 4496, 'practical': 4497, 'discretion': 4498, 'soft': 4499, 'breathe': 4500, 'dragon': 4501, 'oil': 4502, 'heating': 4503, 'silky': 4504, 'colour': 4505, 'Olivia': 4506, 'cherry-': 4507, 'satin': 4508, 'pot': 4509, 'beadwork': 4510, 'carving': 4511, 'Congo': 4512, 'progression': 4513, 'Mackie': 4514, 'knowing': 4515, 'dare': 4516, 'measurement': 4517, 'trouser': 4518, 'elastic': 4519, 'Adamson': 4520, 'Mweta': 4521, 'steak': 4522, 'Kensington': 4523, 'fullness': 4524, 'twilight': 4525, 'housing': 4526, 'estate': 4527, 'overrun': 4528, 'reverse': 4529, 'manor': 4530, 'priory': 4531, 'nineteenth': 4532, 'depopulate': 4533, 'industrialize': 4534, 'autonomy': 4535, 'cum': 4536, 'cottage': 4537, 'renewal': 4538, 'reassurance': 4539, 'stubborn': 4540, 'fecundity': 4541, 'daughter': 4542, 'independence': 4543, 'confer': 4544, 'Colonial': 4545, 'smooth': 4546, 'delegation': 4547, 'central': 4548, 'territory': 4549, 'colonial': 4550, 'succeed': 4551, 'recall': 4552, 'deport': 4553, 'People': 4554, 'Independence': 4555, 'Party': 4556, 'guest': 4557, 'celebration': 4558, 'marvellous': 4559, 'cycle': 4560, 'Gala': 4561, 'province': 4562, 'weekend': 4563, 'meeting': 4564, 'eld': 4565, 'shy': 4566, 'powerful': 4567, 'Venetia': 4568, 'Englishman': 4569, 'Bray': 4570, 'baby': 4571, \"celebrations'll\": 4572, 'taxi': 4573, 'uprush': 4574, 'quick': 4575, 'bicep': 4576, 'constitutional': 4577, 'tactic': 4578, 'impersonal': 4579, 'consciousness': 4580, 'stimulant': 4581, 'inject': 4582, 'ours': 4583, 'faÃ§ade': 4584, 'lintel': 4585, 'sill': 4586, 'soap': 4587, 'shelter': 4588, 'grassy': 4589, 'fuzzy': 4590, 'moth': 4591, 'desultorily': 4592, 'pull': 4593, 'rank': 4594, 'weed': 4595, 'yield': 4596, 'humus': 4597, 'crumb': 4598, 'underground': 4599, 'cake': 4600, 'walnut': 4601, 'damp': 4602, 'whisky': 4603, 'waver': 4604, 'golden': 4605, 'meadow': 4606, 'partridge': 4607, 'fade': 4608, 'convey': 4609, 'satisfaction': 4610, 'Stravinsky': 4611, 'Poulenc': 4612, 'knit': 4613, 'grandmother': 4614, 'stuff': 4615, 'niece': 4616, 'nephew': 4617, 'dissemble': 4618, 'confront': 4619, 'glibness': 4620, 'posit': 4621, 'calmly': 4622, 'youngish': 4623, 'schoolgirl': 4624, 'historically': 4625, 'possession': 4626, 'quit': 4627, 'pride': 4628, 'victory': 4629, 'Henry': 4630, 'Davis': 4631, 'M.P.': 4632, 'banish': 4633, 'Province': 4634, 'marriage': 4635, 'deeply': 4636, 'Wiltshire': 4637, 'definitive': 4638, 'Englishwoman': 4639, 'storage': 4640, 'furniture': 4641, 'inevitably': 4642, 'arrangement': 4643, 'grandfather': 4644, 'morocco': 4645, 'erase': 4646, 'boxwood': 4647, 'empirical': 4648, 'scribble': 4649, 'scented': 4650, 'mothy': 4651, 'premonition': 4652, 'lover': 4653, 'stock': 4654, 'discovery': 4655, 'tobacco': 4656, 'sensible': 4657, 'inquire': 4658, 'misunderstand': 4659, 'usually': 4660, 'Good': 4661, 'Lord': 4662, 'unexpectedly': 4663, 'Mozart': 4664, 'harp': 4665, 'Mute': 4666, 'concerto': 4667, 'herb': 4668, 'branch': 4669, 'dill': 4670, 'youngster': 4671, 'hatch': 4672, 'barrel': 4673, 'steady': 4674, 'bleary': 4675, 'shopwindow': 4676, 'blurred': 4677, 'Athens': 4678, 'icy': 4679, 'wet': 4680, 'Aegean': 4681, 'thyme': 4682, 'crinkly': 4683, 'grey': 4684, 'filthy': 4685, 'cleaning': 4686, 'ease': 4687, 'cramp': 4688, 'knee': 4689, 'circumference': 4690, 'oneself': 4691, 'embroider': 4692, 'apron': 4693, 'evzone': 4694, 'badge': 4695, 'canton': 4696, 'Switzerland': 4697, 'sew': 4698, 'postcard': 4699, 'dazzle': 4700, 'Cambridge': 4701, 'spre': 4702, 'yell': 4703, 'mistake': 4704, 'Kano': 4705, 'moon': 4706, 'bright': 4707, 'tarmac': 4708, 'resistance': 4709, 'persist': 4710, 'woodsmoke': 4711, 'beneath': 4712, 'belly': 4713, 'bare': 4714, 'aboard': 4715, 'airless': 4716, 'hamper': 4717, 'rearrange': 4718, 'gear': 4719, 'anticipation': 4720, 'arrival': 4721, 'arouse': 4722, 'instinct': 4723, 'herd': 4724, 'slump': 4725, 'sleep': 4726, 'beam': 4727, 'hairnet': 4728, 'stubble': 4729, 'immigration': 4730, 'BRAY': 4731, 'Evelyn': 4732, 'passport': 4733, 'flex': 4734, 'awkwardly': 4735, 'mild': 4736, 'embarrassment': 4737, 'notch': 4738, 'sponge': 4739, 'vacant': 4740, 'greet': 4741, 'doze': 4742, 'intimate': 4743, 'cage': 4744, 'wedge': 4745, 'distant': 4746, 'apologetically': 4747, 'cologne': 4748, 'freckle': 4749, 'curtain': 4750, 'oval': 4751, 'glorious': 4752, 'animation': 4753, 'furze': 4754, 'growth': 4755, 'runway': 4756, 'unhook': 4757, 'safety': 4758, 'belt': 4759, 'angle': 4760, 'lens': 4761, 'distorted': 4762, 'thinking': 4763, 'underfoot': 4764, 'khaki': 4765, 'stocking': 4766, 'spray': 4767, 'cloyingly': 4768, 'insecticide': 4769, 'precaution': 4770, 'harbouring': 4771, 'mosquito': 4772, 'tsetse': 4773, 'heady': 4774, 'potato': 4775, 'undergrowth': 4776, 'warmth': 4777, 'cool': 4778, 'metallic': 4779, 'taste': 4780, 'storm': 4781, 'throat': 4782, 'disembarking': 4783, 'mouthing': 4784, 'balcony': 4785, 'processional': 4786, 'reception': 4787, 'insect': 4788, 'sweep': 4789, 'summon': 4790, 'booth': 4791, 'companion': 4792, 'convention': 4793, 'insist': 4794, 'Colonel': 4795, 'coarse': 4796, 'strand': 4797, 'ear': 4798, 'sunglass': 4799, 'nordic': 4800, 'cheekbone': 4801, 'baldish': 4802, 'accent': 4803, 'recently': 4804, 'Hjalmar': 4805, 'Wentz': 4806, 'rhino': 4807, 'Denmark': 4808, 'charcoal': 4809, 'grill': 4810, 'whatnot': 4811, 'McGowan': 4812, 'miner': 4813, 'pub': 4814, 'overawing': 4815, 'genteel': 4816, 'gentle': 4817, 'lamb': 4818, 'Barry': 4819, 'Forsyth': 4820, 'Construction': 4821, 'Isoza': 4822, 'River': 4823, 'reclamation': 4824, 'scheme': 4825, 'Poland': 4826, 'spider': 4827, 'flatten': 4828, 'starfish': 4829, 'fat': 4830, 'intensely': 4831, 'sociable': 4832, 'pitch': 4833, 'Dando': 4834, 'cook': 4835, 'Festus': 4836, 'fridge': 4837, 'pink': 4838, 'specially': 4839, 'muddle': 4840, 'obstinate': 4841, 'righteously': 4842, 'maid': 4843, 'retail': 4844, 'interruption': 4845, 'exaggeration': 4846, 'paternalist': 4847, 'discipline': 4848, 'District': 4849, 'district': 4850, 'magistrate': 4851, 'ambulance': 4852, 'instance': 4853, 'Public': 4854, 'Health': 4855, 'D.C.': 4856, 'dependency': 4857, 'resentment': 4858, 'worry': 4859, 'Bench': 4860, 'bottomless': 4861, 'distaste': 4862, 'wrinkle': 4863, 'mug': 4864, 'breed': 4865, 'Gwenzi': 4866, 'Inn': 4867, 'African': 4868, 'deportation': 4869, 'warrant': 4870, 'arrest': 4871, 'mouthful': 4872, 'granadilla': 4873, 'pudding': 4874, 'tremor': 4875, 'dando': 4876, 'fool': 4877, 'Prosecutor': 4878, 'Shinza': 4879, 'bloody': 4880, 'discount': 4881, 'opposition': 4882, 'trick': 4883, 'PIP': 4884, 'Lambala': 4885, 'boycott': 4886, 'beatingsup': 4887, 'poll': 4888, 'burning': 4889, 'contented': 4890, 'loud': 4891, 'refill': 4892, 'brandy': 4893, 'confident': 4894, 'verdict': 4895, 'biased': 4896, 'midnight': 4897, 'cockroach': 4898, 'flee': 4899, 'twirl': 4900, 'antennae': 4901, 'furry': 4902, 'band': 4903, 'cupboard': 4904, 'flick': 4905, 'plate': 4906, 'avocado': 4907, 'pear': 4908, 'pip': 4909, 'matchstick': 4910, 'pickle': 4911, 'jar': 4912, 'giddy': 4913, 'volition': 4914, 'Labrador': 4915, 'reproachfully': 4916, 'assistant': 4917, 'tea': 4918, 'snore': 4919, 'greeting': 4920, 'expunge': 4921, 'satisfied': 4922, 'symbol': 4923, 'inseparable': 4924, 'relevant': 4925, 'stadium': 4926, 'perimeter': 4927, 'Kenyatta': 4928, 'Vivien': 4929, 'Bayley': 4930, 'registrar': 4931, 'university': 4932, 'collide': 4933, 'alert': 4934, 'apprehension': 4935, 'flip': 4936, 'crown': 4937, 'publicity': 4938, 'stunt': 4939, 'Neil': 4940, 'visitor': 4941, 'bray': 4942, 'tension': 4943, 'couple': 4944, 'tier': 4945, 'canopy': 4946, 'dai': 4947, 'photographer': 4948, 'solemnity': 4949, 'bent': 4950, 'tiptoe': 4951, 'thrust': 4952, 'contraption': 4953, 'shutter': 4954, 'flashlight': 4955, 'ready': 4956, 'theatrical': 4957, 'performance': 4958, 'workman': 4959, 'risen': 4960, 'temper': 4961, 'distraction': 4962, 'disorderly': 4963, 'symbolic': 4964, 'attainment': 4965, 'willed': 4966, 'roar': 4967, 'interval': 4968, 'togas': 4969, 'medal': 4970, 'ululate': 4971, 'clash': 4972, 'brass': 4973, 'icecream': 4974, 'tricycle': 4975, 'amphitheatre': 4976, 'mongrel': 4977, 'presidential': 4978, 'mummify': 4979, 'vessel': 4980, 'ritual': 4981, 'irresistibly': 4982, 'spectator': 4983, 'spectacle': 4984, 'embarrassed': 4985, 'elderly': 4986, 'english': 4987, 'princess': 4988, 'neatly': 4989, 'Royal': 4990, 'curiously': 4991, 'contingent': 4992, 'whiten': 4993, 'joy': 4994, 'troop': 4995, 'musician': 4996, 'dancer': 4997, 'occasion': 4998, 'Ball': 4999, 'cocktail': 5000, 'banquet': 5001, 'luncheon': 5002, 'mood': 5003, 'palace': 5004, 'gate': 5005, 'Roly': 5006, 'mock': 5007, 'surprise': 5008, 'spontaneously': 5009, 'Bayleys': 5010, 'Clough': 5011, 'governor': 5012, 'junior': 5013, 'Tanganyika': 5014, 'Cyprian': 5015, 'Kente': 5016, 'Interior': 5017, 'Tindi': 5018, 'Timothy': 5019, 'Odara': 5020, 'Poles': 5021, 'Ghanaians': 5022, 'Hungarians': 5023, 'refugee': 5024, 'marquee': 5025, 'congolese': 5026, 'whip': 5027, 'thrilling': 5028, 'cosy': 5029, 'gaiety': 5030, 'tent': 5031, 'divan': 5032, 'collage': 5033, 'yawning': 5034, 'curiosity': 5035, 'threaten': 5036, 'finery': 5037, 'queenly': 5038, 'mannered': 5039, 'permutation': 5040, 'drunk': 5041, 'overlook': 5042, 'stiffly': 5043, 'gyration': 5044, 'ashamed': 5045, 'politeness': 5046, 'vanity': 5047, 'flirt': 5048, 'wriggle': 5049, 'Andrew': 5050, 'friendliness': 5051, 'relationship': 5052, 'expose': 5053, 'crowded': 5054, 'purse': 5055, 'spare': 5056, 'stomach': 5057, 'heighten': 5058, 'neighbour': 5059, 'fingerprint': 5060, 'relinquish': 5061, 'Ras': 5062, 'Asahe': 5063, 'broadcast': 5064, 'Services': 5065, 'Joseph': 5066, 'Edward': 5067, 'lieutenant': 5068, 'suggestion': 5069, 'cufflink': 5070, 'Mussolini': 5071, 'jaw': 5072, 'lyrical': 5073, 'delicately': 5074, 'blandness': 5075, 'businessman': 5076, 'marvel': 5077, 'brutalizing': 5078, 'hardship': 5079, 'coverage': 5080, 'classroom': 5081, 'considerably': 5082, 'shortage': 5083, 'qualified': 5084, 'keen': 5085, 'General': 5086, 'serving': 5087, 'roast': 5088, 'sheep': 5089, 'stocky': 5090, 'Roland': 5091, 'Rhino': 5092, 'Margot': 5093, 'fuss': 5094, 'spit': 5095, 'shiny': 5096, 'forehead': 5097, 'capable': 5098, 'tidbit': 5099, 'crisp': 5100, 'literally': 5101, 'wellshape': 5102, 'eager': 5103, 'justice': 5104, 'labour': 5105, 'us': 5106, 'softly': 5107, 'majestically': 5108, 'd': 5109, 'mango': 5110, 'crazy': 5111, 'excitedly': 5112, 'slighter': 5113, 'proprietor': 5114, 'Silver': 5115, 'anyway': 5116, 'ugliness': 5117, 'noone': 5118, 'contempt': 5119, 'Stephen': 5120, 'amazing': 5121, 'education': 5122, 'glimmer': 5123, 'innocently': 5124, 'rejection': 5125, 'chilly': 5126, 'indoor': 5127, 'fireplace': 5128, 'bang': 5129, 'Governor': 5130, 'panga': 5131, 'picannin': 5132, 'snotty': 5133, 'sulky': 5134, 'department': 5135, 'glittering': 5136, 'Doris': 5137, 'Manyema': 5138, 'graduate': 5139, 'appreciation': 5140, 'apart': 5141, 'blackness': 5142, 'figurehead': 5143, 'hulk': 5144, 'chin': 5145, 'handsome': 5146, 'magnificent': 5147, 'underline': 5148, 'fatigue': 5149, 'absent': 5150, 'colonialism': 5151, 'indefensible': 5152, 'shanty': 5153, 'dirt': 5154, 'alphabet': 5155, 'malaria': 5156, 'Finns': 5157, 'Russians': 5158, 'anybody': 5159, 'sweat': 5160, 'luckily': 5161, 'colonialist': 5162, 'compound': 5163, 'neo': 5164, 'Curtis': 5165, 'ach': 5166, 'healthy': 5167, 'missionary': 5168, 'Pettigrew': 5169, 'Europeans': 5170, 'reasonably': 5171, 'grimace': 5172, 'mistaken': 5173, 'grandly': 5174, 'Hitler': 5175, 'tooth': 5176, 'impatient': 5177, 'pleasantness': 5178, 'killing': 5179, 'slave': 5180, 'eighteenth': 5181, 'gipsy': 5182, 'seventeenth': 5183, 'birthday': 5184, 'Fort': 5185, 'Howard': 5186, 'Majesty': 5187, 'Auschwitz': 5188, 'Jo': 5189, 'Ann': 5190, 'blob': 5191, 'marshmallow': 5192, 'countryman': 5193, 'assumption': 5194, 'angrily': 5195, 'waggle': 5196, 'sticky': 5197, 'handkerchief': 5198, 'Pettigrews': 5199, 'Sputnik': 5200, 'Bar': 5201, 'Odaras': 5202, 'Wentzes': 5203, 'Rebecca': 5204, 'terrific': 5205, 'hey': 5206, 'Laughter': 5207, 't': 5208, 'enthusiasm': 5209, 'spring': 5210, 'diademe': 5211, 'raindrop': 5212, 'anthropologist': 5213, 'ras': 5214, 'Edwards': 5215, 'naturedly': 5216, 'inquiringly': 5217, 'film': 5218, 'INDEPENDENCE': 5219, 'HURRAH': 5220, 'hers': 5221, 'desperate': 5222, 'buffeting': 5223, 'chase': 5224, 'moonlit': 5225, 'Bwana': 5226, 'hedgehog': 5227, 'pinkness': 5228, 'aggression': 5229, 'moonlight': 5230, 'beast': 5231, 'donkey': 5232, 'cropping': 5233, 'china': 5234, 'mosque': 5235, 'silvered': 5236, 'burglar': 5237, 'indian': 5238, 'haphazardly': 5239, 'crop': 5240, 'plunge': 5241, 'Privet': 5242, 'Vernon': 5243, 'Dursley': 5244, 'hoot': 5245, 'Harry': 5246, 'Petunia': 5247, 'belch': 5248, 'Dursleys': 5249, 'Dudley': 5250, 'feed': 5251, 'droop': 5252, 'sentence': 5253, 'scream': 5254, 'clap': 5255, 'throbbing': 5256, 'temple': 5257, 'purple': 5258, 'uncle': 5259, 'aunt': 5260, 'heave': 5261, 'Uncle': 5262, 'rhinoceros': 5263, 'holiday': 5264, 'Potter': 5265, 'normal': 5266, 'wizard': 5267, 'Hogwarts': 5268, 'School': 5269, 'Witchcraft': 5270, 'Wizardry': 5271, 'miss': 5272, 'hogwart': 5273, 'ache': 5274, 'spellbook': 5275, 'wand': 5276, 'robe': 5277, 'cauldron': 5278, 'Nimbus': 5279, 'Two': 5280, 'Thousand': 5281, 'broomstick': 5282, 'stair': 5283, 'Quidditch': 5284, 'team': 5285, 'homework': 5286, 'muggle': 5287, 'magical': 5288, 'shame': 5289, 'padlock': 5290, 'Hedwig': 5291, 'message': 5292, 'wizarde': 5293, 'neckless': 5294, 'moustache': 5295, 'Aunt': 5296, 'bony': 5297, 'porky': 5298, 'skinny': 5299, 'jet': 5300, 'untidy': 5301, 'unusual': 5302, 'dursley': 5303, 'doorstep': 5304, 'curse': 5305, 'sorcer': 5306, 'Voldemort': 5307, 'witch': 5308, 'voldemort': 5309, 'destroy': 5310, 'smelly': 5311, 'twelfth': 5312, 'career': 5313, 'bitterly': 5314, 'fortnight': 5315, 'builder': 5316, 'drill': 5317, 'simper': 5318, 'Excellent': 5319, 'viciously': 5320, 'Mason': 5321, 'hug': 5322, 'duck': 5323, 'forcefully': 5324, 'lounge': 5325, 'luck': 5326, 'Ten': 5327, 'excited': 5328, 'Majorca': 5329, 'lawn': 5330, 'card': 5331, 'miserably': 5332, 'hedge': 5333, 'Ron': 5334, 'Weasley': 5335, 'Hermione': 5336, 'Granger': 5337, 'countless': 5338, 'unlock': 5339, 'magic': 5340, 'underage': 5341, 'terror': 5342, 'dung': 5343, 'beetle': 5344, 'arch': 5345, 'Draco': 5346, 'Malfoy': 5347, 'fun': 5348, 'terrifying': 5349, 'cunning': 5350, 'determined': 5351, 'regain': 5352, 'clutch': 5353, 'drench': 5354, 'livid': 5355, 'bolt': 5356, 'upright': 5357, 'mindedly': 5358, 'jeering': 5359, 'hitch': 5360, 'suspiciously': 5361, 'stumble': 5362, 'backwards': 5363, 'panic': 5364, 'MUUUUUUM': 5365, 'howl': 5366, 'dearly': 5367, 'hurt': 5368, 'soapy': 5369, 'fry': 5370, 'pan': 5371, 'loll': 5372, 'mow': 5373, 'trim': 5374, 'flowerbeds': 5375, 'prune': 5376, 'rose': 5377, 'repaint': 5378, 'overhead': 5379, 'burn': 5380, 'bait': 5381, 'Wish': 5382, 'savagely': 5383, 'manure': 5384, 'gladly': 5385, 'sugar': 5386, 'violet': 5387, 'pork': 5388, 'sizzle': 5389, 'oven': 5390, 'salmon': 5391, 'supper': 5392, 'whisk': 5393, 'upstairs': 5394, 'landing': 5395, 'bell': 5396, 'creature': 5397, 'bat': 5398, 'bulge': 5399, 'tennis': 5400, 'instantly': 5401, 'pillowcase': 5402, 'rip': 5403, 'th': 5404, 'Dobby': 5405, 'er': 5406, 'rude': 5407, 'elf': 5408, 'Petunias': 5409, 'false': 5410, 'starve': 5411, 'Moonlight': 5412, 'nosed': 5413, 'impact': 5414, 'turquoise': 5415, 'Fred': 5416, 'George': 5417, 'twin': 5418, 'gibber': 5419, 'tightly': 5420, 'rev': 5421, 'realise': 5422, 'louder': 5423, 'crunching': 5424, 'panting': 5425, 'anxiously': 5426, 'hairpin': 5427, 'click': 5428, 'creak': 5429, 'cough': 5430, 'slide': 5431, 'thunder': 5432, 'snatch': 5433, 'scramble': 5434, 'drawer': 5435, 'hammer': 5436, 'unlocked': 5437, 'split': 5438, 'doorway': 5439, 'bellow': 5440, 'angry': 5441, 'bull': 5442, 'dive': 5443, 'ankle': 5444, 'seize': 5445, 'Weasleys': 5446, 'shrink': 5447, 'rooftop': 5448, 'dumbstruck': 5449, 'soar': 5450, 'joyfully': 5451, 'alongside': 5452, 'ghost': 5453, 'fiasco': 5454, 'shocked': 5455, 'definitely': 5456, 'dodgy': 5457, 'master': 5458, 'permission': 5459, 'reckon': 5460, 'supporter': 5461, 'rumour': 5462, 'yeah': 5463, 'Mum': 5464, 'ironing': 5465, 'strut': 5466, 'Errol': 5467, 'fault': 5468, 'delivery': 5469, 'Dad': 5470, 'Percy': 5471, 'compass': 5472, 'dashboard': 5473, 'twiddle': 5474, 'steering': 5475, 'boring': 5476, 'antique': 5477, 'Muggle': 5478, 'pinkish': 5479, 'horizon': 5480, 'patchwork': 5481, 'Touchdown': 5482, 'tumbledown': 5483, 'garage': 5484, 'pigsty': 5485, 'extra': 5486, 'storey': 5487, 'chimney': 5488, 'perch': 5489, 'jumble': 5490, 'wellington': 5491, 'downstairs': 5492, 'greenish': 5493, 'plump': 5494, 'remarkable': 5495, 'saber': 5496, 'toothe': 5497, 'tiger': 5498, 'halt': 5499, 'cower': 5500, 'Bill': 5501, 'hoarse': 5502, 'nod': 5503, 'encouragingly': 5504, 'scrubbed': 5505, 'clock': 5506, 'mantelpiece': 5507, 'Charm': 5508, 'Cheese': 5509, 'enchantment': 5510, 'Baking': 5511, 'Minute': 5512, 'Feasts': 5513, 'clatter': 5514, 'sausage': 5515, 'Arthur': 5516, 'Friday': 5517, 'casually': 5518, 'washing': 5519, 'softened': 5520, 'bread': 5521, 'diversion': 5522, 'figure': 5523, 'nightdress': 5524, 'squeal': 5525, 'surprisingly': 5526, 'Blimey': 5527, 'yawn': 5528, 'groan': 5529, 'Gilderoy': 5530, 'Lockhart': 5531, 'Guide': 5532, 'Household': 5533, 'Pests': 5534, 'molly': 5535, 'Floo': 5536, 'powder': 5537, 'pinch': 5538, 'breeze': 5539, 'swallow': 5540, 'ash': 5541, 'giant': 5542, 'plug': 5543, 'roaring': 5544, 'deafen': 5545, 'whirl': 5546, 'tuck': 5547, 'squint': 5548, 'bacon': 5549, 'churn': 5550, 'Dizzy': 5551, 'bruise': 5552, 'gingerly': 5553, 'dimly': 5554, 'withered': 5555, 'stain': 5556, 'assortment': 5557, 'spiked': 5558, 'instrument': 5559, 'ceiling': 5560, 'dusty': 5561, 'Diagon': 5562, 'Alley': 5563, 'cabinet': 5564, 'peer': 5565, 'clang': 5566, 'identical': 5567, 'racing': 5568, 'broom': 5569, 'drum': 5570, 'Dumbledore': 5571, 'Gryffindor': 5572, 'shelf': 5573, 'skull': 5574, 'Borgin': 5575, 'assistance': 5576, 'Ministry': 5577, 'parchment': 5578, 'unravel': 5579, 'pince': 5580, 'nez': 5581, 'curl': 5582, 'meddlesome': 5583, 'surge': 5584, 'poison': 5585, 'Insert': 5586, 'thief': 5587, 'plunderer': 5588, 'favourite': 5589, 'flaring': 5590, 'haggle': 5591, 'nervously': 5592, 'hiding': 5593, 'coil': 5594, 'hangman': 5595, 'smirk': 5596, 'prop': 5597, 'necklace': 5598, 'opal': 5599, 'caution': 5600, 'Owners': 5601, 'Date': 5602, 'Come': 5603, 'darkly': 5604, 'dingy': 5605, 'alleyway': 5606, 'devote': 5607, 'Dark': 5608, 'Arts': 5609, 'Burkes': 5610, 'shrunken': 5611, 'shabby': 5612, 'jumpy': 5613, 'poisonous': 5614, 'Knockturn': 5615, 'ashe': 5616, 'aged': 5617, 'fingernail': 5618, 'leer': 5619, 'mossy': 5620, 'leapt': 5621, 'Hagrid': 5622, 'scruff': 5623, 'shriek': 5624, 'twisting': 5625, 'snow': 5626, 'marble': 5627, 'Gringotts': 5628, 'Bank': 5629, 'steer': 5630, 'yer': 5631, 'gruffly': 5632, 'brushing': 5633, 'apothecary': 5634, 'ruinin': 5635, 'cabbage': 5636, 'Ruddy': 5637, 'Muggles': 5638, 'growl': 5639, 'bushy': 5640, 'Oh': 5641, 'sprint': 5642, 'mop': 5643, 'ruddy': 5644, 'came': 5645, 'gallop': 5646, 'handbag': 5647, 'swinge': 5648, 'wildly': 5649, 'Ginny': 5650, 'tap': 5651, 'Lucius': 5652, 'tenpound': 5653, 'vault': 5654, 'goblin': 5655, 'cart': 5656, 'miniature': 5657, 'breakneck': 5658, 'dreadful': 5659, 'pile': 5660, 'Sickles': 5661, 'Galleon': 5662, 'shove': 5663, 'handful': 5664, 'coin': 5665, 'leather': 5666, 'quill': 5667, 'Lee': 5668, 'Jordan': 5669, 'Grangers': 5670, 'Leaky': 5671, 'Cauldron': 5672, 'winding': 5673, 'cobble': 5674, 'bronze': 5675, 'jangling': 5676, 'cheerfully': 5677, 'clamour': 5678, 'strawberry': 5679, 'peanut': 5680, 'slurp': 5681, 'happily': 5682, 'alley': 5683, 'fascinating': 5684, 'aloud': 5685, 'Flourish': 5686, 'Blotts': 5687, 'bookshop': 5688, 'jostle': 5689, 'proclaim': 5690, 'banner': 5691, 'GILDEROY': 5692, 'LOCKHART': 5693, 'autobiography': 5694, 'MAGICAL': 5695, 'ME': 5696, '12:30': 5697, '4:30': 5698, 'pm': 5699, 'booklist': 5700, 'Break': 5701, 'Banshee': 5702, 'sneak': 5703, 'breathless': 5704, 'pat': 5705, 'wink': 5706, 'dazzlingly': 5707, 'jaunty': 5708, 'wavy': 5709, 'camera': 5710, 'emit': 5711, 'bunione': 5712, 'bulrush': 5713, 'butt': 5714, 'tenacious': 5715, 'buckaroo': 5716, 'Wheel': 5717, 'Fortune': 5718, 'dizzy': 5719, 'Stella': 5720, 'Honest': 5721, 'Guv': 5722, 'cope': 5723, '1960': 5724, 'Skirts': 5725, 'orange': 5726, 'vampire': 5727, 'Matisse': 5728, 'peculiarly': 5729, 'assault': 5730, 'Liverpool': 5731, 'slumber': 5732, 'Sixties': 5733, 'Beatles': 5734, 'victim': 5735, 'Merseybeat': 5736, 'whelp': 5737, 'blanket': 5738, 'noctivagant': 5739, 'tow': 5740, 'seriousness': 5741, 'emotion': 5742, 'easily': 5743, 'relief': 5744, 'Daddy': 5745, 'Grandmother': 5746, 'woollen': 5747, 'balaclava': 5748, 'cocoa': 5749, 'daze': 5750, 'massacre': 5751, 'amputate': 5752, 'Snow': 5753, 'scissored': 5754, 'chrome': 5755, 'shrine': 5756, 'Chanel': 5757, 'flimsy': 5758, 'barrier': 5759, 'policeman': 5760, 'lorry': 5761, 'excite': 5762, 'impress': 5763, 'Alice': 5764, 'pill': 5765, 'gin': 5766, 'maddening': 5767, 'magnet': 5768, 'diner': 5769, 'menacingly': 5770, 'nice': 5771, 'cloth': 5772, 'carnation': 5773, 'rod': 5774, 'grissini': 5775, 'carafe': 5776, 'olive': 5777, 'Borgias': 5778, 'drain': 5779, 'Uncharacteristically': 5780, 'complacent': 5781, 'Zeus': 5782, 'yowl': 5783, 'firmament': 5784, 'Hermes': 5785, 'Hephaestus': 5786, 'lame': 5787, 'god': 5788, 'smithy': 5789, 'Athene': 5790, 'bore': 5791, '1959': 5792, 'guitar': 5793, 'drumstick': 5794, 'Husband': 5795, 'herring': 5796, 'mink': 5797, 'pearl': 5798, 'transvestite': 5799, 'silk': 5800, 'manifest': 5801, 'onion': 5802, 'torture': 5803, 'twisted': 5804, 'kipper': 5805, 'skewer': 5806, 'pair': 5807, 'knitting': 5808, 'coal': 5809, 'homemade': 5810, 'rabbit': 5811, 'bobbed': 5812, 'buckle': 5813, 'lid': 5814, 'hobby': 5815, 'underwater': 5816, 'rare': 5817, 'unexplained': 5818, 'Tha': 5819, 'birth': 5820, 'resourceful': 5821, 'soup': 5822, 'christen': 5823, 'frost': 5824, 'petrified': 5825, 'frozen': 5826, 'headmaster': 5827, 'secondary': 5828, 'inattentive': 5829, 'rugger': 5830, 'square': 5831, 'misery': 5832, 'agony': 5833, 'windscreen': 5834, 'prey': 5835, 'devilishly': 5836, 'sycamore': 5837, 'enamel': 5838, 'unmatched': 5839, 'Rayburn': 5840, 'scone': 5841, 'nightcase': 5842, 'disinfectant': 5843, 'honorary': 5844, 'toilet': 5845, 'strictly': 5846, 'forbid': 5847, 'monthly': 5848, 'dread': 5849, 'drunker': 5850, 'reluctant': 5851, 'unhygienic': 5852, 'foundation': 5853, 'Tupperware': 5854, \"why?'and\": 5855, 'mob': 5856, 'Southampton': 5857, 'pouch': 5858, 'faced': 5859, 'narcotic': 5860, 'promising': 5861, 'addicted': 5862, 'sentimentality': 5863, 'cruel': 5864, 'cruelty': 5865, 'observation': 5866, 'unable': 5867, 'bon': 5868, 'viveur': 5869, 'recipe': 5870, 'canapÃ©s': 5871, 'relative': 5872, 'dock': 5873, 'loading': 5874, 'bay': 5875, '8': 5876, 'Sun': 5877, 'Gemini': 5878, 'whilst': 5879, 'adjust': 5880, 'apparition': 5881, 'Twist': 5882, 'Wiggle': 5883, 'mobile': 5884, 'plant': 5885, 'brand': 5886, 'Danette': 5887, 'turntable': 5888, '45r.p.m': 5889, 'singing': 5890, 'scoop': 5891, 'dunny': 5892, 'sawdust': 5893, 'midden': 5894, 'memorial': 5895, 'oilskin': 5896, 'wheeze': 5897, 'whir': 5898, 'scratching': 5899, 'fluff': 5900, 'player': 5901, 'ITALIAN': 5902, 'inherit': 5903, 'crieth': 5904, 'Aha': 5905, 'wrestle': 5906, 'angel': 5907, 'unfixed': 5908, 'spine': 5909, 'solemnly': 5910, 'vision': 5911, 'intent': 5912, 'reveal': 5913, 'religion': 5914, 'superstition': 5915, 'Original': 5916, 'Sin': 5917, 'unhappiness': 5918, 'gothic': 5919, 'disposition': 5920, 'mercy': 5921, 'hood': 5922, 'upside': 5923, 'genital': 5924, 'stethoscope': 5925, 'resent': 5926, 'overweight': 5927, 'careless': 5928, 'poetically': 5929, 'besocke': 5930, 'sandalle': 5931, 'museum': 5932, 'exhibit': 5933, 'flesh': 5934, 'escaping': 5935, 'trap': 5936, 'sump': 5937, 'did': 5938, 'sanitation': 5939, 'eczema': 5940, 'asthma': 5941, 'allergic': 5942, 'reaction': 5943, 'itchy': 5944, 'bleach': 5945, 'drug': 5946, 'fume': 5947, 'nylon': 5948, 'athlete': 5949, 'weal': 5950, 'lace': 5951, 'knicker': 5952, 'secular': 5953, 'papa': 5954, 'beckon': 5955, 'unwatched': 5956, 'navigate': 5957, 'confession': 5958, 'broil': 5959, 'frighten': 5960, 'scarf': 5961, 'idle': 5962, 'poverty': 5963, 'Admiral': 5964, 'Arms': 5965, 'ration': 5966, 'unpaid': 5967, 'desperately': 5968, 'sixth': 5969, 'suitcase': 5970, 'jigsaw': 5971, 'inadequate': 5972, 'melt': 5973, 'icicle': 5974, 'kiss': 5975, 'seam': 5976, 'picnic': 5977, 'whore': 5978, 'slacken': 5979, 'cocked': 5980, 'snap': 5981, 'shovel': 5982, 'compost': 5983, 'cloaca': 5984, 'mould': 5985, 'ballad': 5986, 'swig': 5987, 'unmarked': 5988, 'billy': 5989, 'grog': 5990, 'blossom': 5991, 'generosity': 5992, 'travel': 5993, 'recklessness': 5994, 'inevitable': 5995, 'prefer': 5996, 'shipping': 5997, 'unsustainable': 5998, 'loss': 5999, 'Trident': 6000, 'Shipping': 6001, 'fund': 6002, '1809': 6003, 'bankrupt': 6004, 'director': 6005, 'celebrate': 6006, 'conceive': 6007, 'unpack': 6008, 'Bible': 6009, 'marker': 6010, 'Book': 6011, 'Job': 6012, 'tribulation': 6013, 'intensity': 6014, 'indifferent': 6015, 'undeterred': 6016, 'imitate': 6017, 'soak': 6018, 'paperwork': 6019, 'secretary': 6020, 'Remington': 6021, 'piano': 6022, 'gap': 6023, 'pier': 6024, 'privy': 6025, 'dancing': 6026, 'unnamed': 6027, 'teenage': 6028, 'vinyl': 6029, 'zip': 6030, 'polka': 6031, 'dot': 6032, 'thy': 6033, 'Cunard': 6034, 'illustrious': 6035, 'prestigious': 6036, 'Trafalgar': 6037, 'Investments': 6038, 'flagship': 6039, 'QE2': 6040, 'foie': 6041, 'gras': 6042, 'reorganisation': 6043, 'atlantic': 6044, 'headquarter': 6045, 'pelt': 6046, 'Dior': 6047, 'cruise': 6048, 'comet': 6049, 'Kohoutek': 6050, 'Czech': 6051, 'astronomer': 6052, 'millennium': 6053, 'portent': 6054, 'popular': 6055, 'atheist': 6056, 'adult': 6057, 'certificate': 6058, 'eighteen': 6059, 'lowly': 6060, 'Progress': 6061, 'Tradition': 6062, 'Integrity': 6063, 'boil': 6064, 'bath': 6065, 'packet': 6066, 'flake': 6067, 'grease': 6068, 'success': 6069, 'Battery': 6070, 'Legend': 6071, 'seek': 6072, 'Holy': 6073, 'Grail': 6074, 'forever': 6075, 'conjunction': 6076, 'timelessness': 6077, 'unfathomable': 6078, 'sighing': 6079, 'straighten': 6080, 'uncenturied': 6081, 'unquantified': 6082, 'alchemy': 6083, 'Godspeed': 6084, 'skirt': 6085, 'perfect': 6086, 'SHE': 6087, 'me': 6088, 'esteem': 6089, 'glue': 6090, 'deceit': 6091, 'pattern': 6092, 'coherence': 6093, 'convince': 6094, 'unfrightened': 6095, 'distress': 6096, 'flare': 6097, 'phosphorous': 6098, 'patrol': 6099, 'Miracle': 6100, 'Sardines': 6101, 'Gin': 6102, 'Algonquin': 6103, 'Hotel': 6104, 'Dorothy': 6105, 'Parker': 6106, 'Thurber': 6107, 'Yorker': 6108, '1957': 6109, 'reservation': 6110, 'stubbornness': 6111, 'gene': 6112, 'freezing': 6113, 'riddle': 6114, 'rhinegold': 6115, 'Rhinegold': 6116, 'Wagner': 6117, 'Ring': 6118, 'dabble': 6119, 'harden': 6120, 'frock': 6121, 'vomit': 6122, 'Mother': 6123, 'absurdity': 6124, 'plus': 6125, 'incipient': 6126, 'irresistible': 6127, 'salt': 6128, 'oyster': 6129, 'floral': 6130, 'octave': 6131, 'admire': 6132, 'soprano': 6133, 'singer': 6134, 'heel': 6135, 'warrior': 6136, 'cleavage': 6137, 'insole': 6138, 'opponent': 6139, 'Mama': 6140, 'sevenyear': 6141, 'rattling': 6142, 'fox': 6143, 'trilby': 6144, 'driver': 6145, 'struggle': 6146, 'sleigh': 6147, 'fur': 6148, 'huddle': 6149, 'hurrying': 6150, 'bark': 6151, 'geyser': 6152, 'wolf': 6153, 'sledge': 6154, 'lick': 6155, 'parma': 6156, 'tongue': 6157, 'Genesis': 6158, 'trial': 6159, 'sympathise': 6160, 'avail': 6161, 'gut': 6162, 'symmetry': 6163, 'remora': 6164, 'greek': 6165, 'fisherman': 6166, 'slim': 6167, 'greyhound': 6168, 'contour': 6169, 'starch': 6170, 'Tiffany': 6171, 'dogwood': 6172, 'suppleness': 6173, 'gift': 6174, 'artful': 6175, 'artless': 6176, 'rat': 6177, 'irish': 6178, 'Cork': 6179, 'Annual': 6180, 'Dinner': 6181, 'Dance': 6182, 'vow': 6183, 'romance': 6184, 'persistence': 6185, 'wedding': 6186, 'Ra': 6187, 'dÃ©cor': 6188, 'Merseyside': 6189, 'qua': 6190, 'violent': 6191, 'shuddering': 6192, 'desirer': 6193, 'frankness': 6194, 'desirability': 6195, 'drinker': 6196, 'levitate': 6197, 'tranced': 6198, 'hypnotise': 6199, 'Institute': 6200, 'Advanced': 6201, 'Studies': 6202, 'legend': 6203, 'lust': 6204, 'Titan': 6205, 'Metis': 6206, 'eventually': 6207, 'oracle': 6208, 'overthrough': 6209, 'depose': 6210, 'Kronos': 6211, 'stroke': 6212, 'eclipse': 6213, 'gradual': 6214, 'chilliness': 6215, 'cheat': 6216, 'salary': 6217, 'insufficient': 6218, 'bonus': 6219, 'challenge': 6220, 'puny': 6221, 'achievement': 6222, 'hooded': 6223, 'instructed': 6224, 'bubble': 6225, 'Sunday': 6226, 'elevation': 6227, 'pension': 6228, 'adversary': 6229, 'footlight': 6230, 'performer': 6231, 'German': 6232, 'claque': 6233, 'frightened': 6234, 'detachment': 6235, 'weekly': 6236, 'preposterous': 6237, 'slipper': 6238, 'length': 6239, 'Polo': 6240, 'mint': 6241, 'batter': 6242, 'Oilskin': 6243, 'chessboard': 6244, 'manoeuvre': 6245, 'Martinis': 6246, 'appreciative': 6247, 'sodomise': 6248}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsBor70gV-yZ"
      },
      "source": [
        "### Create the Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 300\n",
        "EMBEDDING_DIM = embeddings.vectors.shape[1]\n",
        "\n",
        "\n",
        "# Get the embedding matrix\n",
        "vocab_size = len(word_index)  # +1 for OOV, +1 for pad\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "# Add OOV and pad vectors\n",
        "embedding_matrix[0] = np.random.uniform(-0.25, 0.25, EMBEDDING_DIM)  # OOV vector\n",
        "embedding_matrix[1] = np.zeros(embeddings.vectors.shape[1])  # pad vector\n",
        "# Add pre-trained embeddings for known words\n",
        "for word, i in word_index.items():\n",
        "  try:\n",
        "      embedding_matrix[i+2] = embeddings[word]  # +2 to account for OOV and pad vectors\n",
        "  except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "-_nn22BHgM0K"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CsL4kFHh3KZ"
      },
      "source": [
        "### Encode the classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X']\n",
        "\n",
        "\n",
        "def mapping(my_list):\n",
        "    mapping = {}\n",
        "    for i, string in enumerate(my_list, start=1): # start from 1 so we don't mix classes with pads\n",
        "        mapping[string] = i\n",
        "    return mapping\n",
        "\n",
        "encoded_classes = mapping(classes)"
      ],
      "metadata": {
        "id": "ZNjGjt1hgPYB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXBHwHMWJoI"
      },
      "source": [
        "### Cut the sentences and encode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cut(sentences):\n",
        "    new=[]\n",
        "    for data in sentences:\n",
        "        new.append(([data[x:x+MAX_SEQUENCE_LENGTH] for x in range(0, len(data), MAX_SEQUENCE_LENGTH)]))\n",
        "    new = [val for sublist in new for val in sublist]\n",
        "    return new\n",
        "\n",
        "train_sentences = cut(train_tuples)\n",
        "dev_sentences = cut(dev_tuples)\n",
        "test_sentences = cut(test_tuples)"
      ],
      "metadata": {
        "id": "uAxPuZRqgUCV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that takes tuples (x, y) and returns the corresponding x_train and y_train \n",
        "def processing_rnn(tuples):\n",
        "  \n",
        "  x = []\n",
        "  y = []\n",
        "  for sent in tuples:\n",
        "    temp_x = []\n",
        "    for word, index in sent:\n",
        "      if word in word_index:\n",
        "        temp_x.append(word_index[word])\n",
        "      else:\n",
        "        temp_x.append(word_index['oov']) # oov\n",
        "    x.append(temp_x)\n",
        "\n",
        "  for sent in tuples:\n",
        "    temp_y = []\n",
        "    for word, index in sent:\n",
        "      if index in encoded_classes:\n",
        "        temp_y.append(encoded_classes[index])\n",
        "      else:\n",
        "        temp_y.append(encoded_classes['X']) # if its something else then tag it as 'X'\n",
        "\n",
        "    y.append(temp_y)\n",
        "\n",
        "\n",
        "  return x, y\n",
        "\n",
        "  \n",
        "x_train_rnn, y_train_rnn = processing_rnn(train_sentences)\n",
        "x_dev_rnn, y_dev_rnn = processing_rnn(dev_sentences) \n",
        "x_test_rnn, y_test_rnn = processing_rnn(test_sentences) "
      ],
      "metadata": {
        "id": "xYBOow36gU8N"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-88_mn7jiGc3"
      },
      "source": [
        "### Generate x_train and 1-hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_train_padded = tf.keras.preprocessing.sequence.pad_sequences(y_train_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_train_cat_padded = to_categorical(y_train_padded, num_classes= len(encoded_classes) + 1)\n",
        "\n",
        "x_dev_padded = tf.keras.preprocessing.sequence.pad_sequences(x_dev_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_dev_padded = tf.keras.preprocessing.sequence.pad_sequences(y_dev_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_dev_cat_padded = to_categorical(y_dev_padded, num_classes= len(encoded_classes) + 1)\n",
        "\n",
        "x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_test_padded = tf.keras.preprocessing.sequence.pad_sequences(y_test_rnn, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "y_test_cat_padded = to_categorical(y_test_padded, num_classes= len(encoded_classes) + 1)"
      ],
      "metadata": {
        "id": "GxcyJ0zYgYI-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln_KI0hpiRuf"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  inputs = keras.Input((x_train_padded.shape[1],),)\n",
        "  mask = keras.layers.Masking(mask_value=0)(inputs) # Mask the pads\n",
        "  embeddings_layer = keras.layers.Embedding(input_dim = vocab_size, output_dim = embeddings.vectors.shape[1],  input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                            weights = [embedding_matrix], mask_zero = True, trainable = True)(mask)\n",
        "  x = keras.layers.BatchNormalization()(embeddings_layer) # Normalize\n",
        "  #x = keras.layers.Bidirectional(keras.layers.LSTM(32, recurrent_dropout=0.4, return_sequences=True))(x) # Layer 1\n",
        "  x = keras.layers.Bidirectional(keras.layers.LSTM(32, recurrent_dropout=0.4, return_sequences=True))(x) # Layer 2\n",
        "  x = keras.layers.Dropout(0.4)(x)\n",
        "  outputs = keras.layers.Dense(y_train_cat_padded.shape[2], activation = 'softmax')(x)\n",
        "  rnn_model = keras.Model(inputs, outputs)\n",
        "\n",
        "  print(rnn_model.summary())\n",
        "\n",
        "  rnn_model.compile(optimizer = SGD(learning_rate=0.1), loss='categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "  if not os.path.exists('./my_RNN_checkpoint'):\n",
        "    os.makedirs('./my_RNN_checkpoint')\n",
        "\n",
        "  rnn_callbacks = [\n",
        "      keras.callbacks.ModelCheckpoint(\n",
        "          './my_RNN_checkpoint/weights.hdf5',\n",
        "          monitor='val_accuracy', \n",
        "          mode='max', \n",
        "          verbose=2,\n",
        "          save_best_only=True,\n",
        "          save_weights_only=True)\n",
        "  ]\n",
        "\n",
        "  history = rnn_model.fit(\n",
        "  x_train_padded,\n",
        "  y_train_cat_padded,\n",
        "  batch_size = 128,\n",
        "  epochs = 40,\n",
        "  validation_data = (x_dev_padded, y_dev_cat_padded),\n",
        "  callbacks= rnn_callbacks,\n",
        "  shuffle = True)\n",
        "\n",
        "\n",
        "  print(f\"Test Accuracy: {rnn_model.evaluate(x_test_padded, y_test_cat_padded)[1]:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko3i4WiIgbj-",
        "outputId": "bd5c3e97-43aa-4ec6-a868-18bb087fc9c2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 300)]             0         \n",
            "                                                                 \n",
            " masking_8 (Masking)         (None, 300)               0         \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, 300, 300)          1874700   \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 300, 300)         1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 300, 64)          85248     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 300, 64)           0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 300, 19)           1235      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,962,383\n",
            "Trainable params: 1,961,783\n",
            "Non-trainable params: 600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 2.3350 - accuracy: 0.3328\n",
            "Epoch 1: val_accuracy improved from -inf to 0.19027, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 77s 3s/step - loss: 2.3350 - accuracy: 0.3328 - val_loss: 2.8419 - val_accuracy: 0.1903\n",
            "Epoch 2/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.7438 - accuracy: 0.5228\n",
            "Epoch 2: val_accuracy improved from 0.19027 to 0.26833, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 1.7438 - accuracy: 0.5228 - val_loss: 2.7357 - val_accuracy: 0.2683\n",
            "Epoch 3/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.4827 - accuracy: 0.5863\n",
            "Epoch 3: val_accuracy did not improve from 0.26833\n",
            "25/25 [==============================] - 70s 3s/step - loss: 1.4827 - accuracy: 0.5863 - val_loss: 2.6250 - val_accuracy: 0.2647\n",
            "Epoch 4/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.3183 - accuracy: 0.6264\n",
            "Epoch 4: val_accuracy improved from 0.26833 to 0.27012, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 1.3183 - accuracy: 0.6264 - val_loss: 2.5137 - val_accuracy: 0.2701\n",
            "Epoch 5/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.2024 - accuracy: 0.6553\n",
            "Epoch 5: val_accuracy did not improve from 0.27012\n",
            "25/25 [==============================] - 68s 3s/step - loss: 1.2024 - accuracy: 0.6553 - val_loss: 2.4132 - val_accuracy: 0.2568\n",
            "Epoch 6/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.1109 - accuracy: 0.6799\n",
            "Epoch 6: val_accuracy did not improve from 0.27012\n",
            "25/25 [==============================] - 68s 3s/step - loss: 1.1109 - accuracy: 0.6799 - val_loss: 2.3227 - val_accuracy: 0.2458\n",
            "Epoch 7/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.6957\n",
            "Epoch 7: val_accuracy improved from 0.27012 to 0.27293, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 1.0437 - accuracy: 0.6957 - val_loss: 2.2151 - val_accuracy: 0.2729\n",
            "Epoch 8/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9836 - accuracy: 0.7136\n",
            "Epoch 8: val_accuracy improved from 0.27293 to 0.38770, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.9836 - accuracy: 0.7136 - val_loss: 2.0907 - val_accuracy: 0.3877\n",
            "Epoch 9/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.9340 - accuracy: 0.7258\n",
            "Epoch 9: val_accuracy improved from 0.38770 to 0.45278, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.9340 - accuracy: 0.7258 - val_loss: 1.9792 - val_accuracy: 0.4528\n",
            "Epoch 10/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8952 - accuracy: 0.7372\n",
            "Epoch 10: val_accuracy improved from 0.45278 to 0.53409, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.8952 - accuracy: 0.7372 - val_loss: 1.8411 - val_accuracy: 0.5341\n",
            "Epoch 11/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8619 - accuracy: 0.7441\n",
            "Epoch 11: val_accuracy improved from 0.53409 to 0.58339, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.8619 - accuracy: 0.7441 - val_loss: 1.6928 - val_accuracy: 0.5834\n",
            "Epoch 12/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.8314 - accuracy: 0.7526\n",
            "Epoch 12: val_accuracy improved from 0.58339 to 0.60804, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.8314 - accuracy: 0.7526 - val_loss: 1.5737 - val_accuracy: 0.6080\n",
            "Epoch 13/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7996 - accuracy: 0.7608\n",
            "Epoch 13: val_accuracy improved from 0.60804 to 0.63036, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 71s 3s/step - loss: 0.7996 - accuracy: 0.7608 - val_loss: 1.4288 - val_accuracy: 0.6304\n",
            "Epoch 14/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7776 - accuracy: 0.7668\n",
            "Epoch 14: val_accuracy improved from 0.63036 to 0.66571, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.7776 - accuracy: 0.7668 - val_loss: 1.2812 - val_accuracy: 0.6657\n",
            "Epoch 15/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7529 - accuracy: 0.7729\n",
            "Epoch 15: val_accuracy improved from 0.66571 to 0.70305, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.7529 - accuracy: 0.7729 - val_loss: 1.1324 - val_accuracy: 0.7031\n",
            "Epoch 16/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7360 - accuracy: 0.7779\n",
            "Epoch 16: val_accuracy did not improve from 0.70305\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.7360 - accuracy: 0.7779 - val_loss: 1.0478 - val_accuracy: 0.6973\n",
            "Epoch 17/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.7826\n",
            "Epoch 17: val_accuracy improved from 0.70305 to 0.74276, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.7175 - accuracy: 0.7826 - val_loss: 0.9397 - val_accuracy: 0.7428\n",
            "Epoch 18/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.7017 - accuracy: 0.7861\n",
            "Epoch 18: val_accuracy improved from 0.74276 to 0.77022, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.7017 - accuracy: 0.7861 - val_loss: 0.8427 - val_accuracy: 0.7702\n",
            "Epoch 19/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6865 - accuracy: 0.7908\n",
            "Epoch 19: val_accuracy did not improve from 0.77022\n",
            "25/25 [==============================] - 71s 3s/step - loss: 0.6865 - accuracy: 0.7908 - val_loss: 0.7925 - val_accuracy: 0.7683\n",
            "Epoch 20/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.7937\n",
            "Epoch 20: val_accuracy improved from 0.77022 to 0.78838, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.6709 - accuracy: 0.7937 - val_loss: 0.7426 - val_accuracy: 0.7884\n",
            "Epoch 21/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.7974\n",
            "Epoch 21: val_accuracy improved from 0.78838 to 0.79840, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.6592 - accuracy: 0.7974 - val_loss: 0.6877 - val_accuracy: 0.7984\n",
            "Epoch 22/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.8035\n",
            "Epoch 22: val_accuracy improved from 0.79840 to 0.80557, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.6446 - accuracy: 0.8035 - val_loss: 0.6561 - val_accuracy: 0.8056\n",
            "Epoch 23/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.8068\n",
            "Epoch 23: val_accuracy improved from 0.80557 to 0.81976, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 70s 3s/step - loss: 0.6304 - accuracy: 0.8068 - val_loss: 0.6183 - val_accuracy: 0.8198\n",
            "Epoch 24/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.8074\n",
            "Epoch 24: val_accuracy improved from 0.81976 to 0.82073, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.6270 - accuracy: 0.8074 - val_loss: 0.6014 - val_accuracy: 0.8207\n",
            "Epoch 25/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.8124\n",
            "Epoch 25: val_accuracy did not improve from 0.82073\n",
            "25/25 [==============================] - 73s 3s/step - loss: 0.6122 - accuracy: 0.8124 - val_loss: 0.5834 - val_accuracy: 0.8190\n",
            "Epoch 26/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.8146\n",
            "Epoch 26: val_accuracy did not improve from 0.82073\n",
            "25/25 [==============================] - 70s 3s/step - loss: 0.6038 - accuracy: 0.8146 - val_loss: 0.5796 - val_accuracy: 0.8181\n",
            "Epoch 27/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.8175\n",
            "Epoch 27: val_accuracy improved from 0.82073 to 0.82780, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.5945 - accuracy: 0.8175 - val_loss: 0.5524 - val_accuracy: 0.8278\n",
            "Epoch 28/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.8195\n",
            "Epoch 28: val_accuracy did not improve from 0.82780\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5844 - accuracy: 0.8195 - val_loss: 0.5532 - val_accuracy: 0.8224\n",
            "Epoch 29/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.8219\n",
            "Epoch 29: val_accuracy did not improve from 0.82780\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5763 - accuracy: 0.8219 - val_loss: 0.5382 - val_accuracy: 0.8262\n",
            "Epoch 30/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.8237\n",
            "Epoch 30: val_accuracy improved from 0.82780 to 0.84533, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5692 - accuracy: 0.8237 - val_loss: 0.5112 - val_accuracy: 0.8453\n",
            "Epoch 31/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.8274\n",
            "Epoch 31: val_accuracy did not improve from 0.84533\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5609 - accuracy: 0.8274 - val_loss: 0.5119 - val_accuracy: 0.8428\n",
            "Epoch 32/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.8291\n",
            "Epoch 32: val_accuracy did not improve from 0.84533\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5546 - accuracy: 0.8291 - val_loss: 0.4985 - val_accuracy: 0.8441\n",
            "Epoch 33/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.8325\n",
            "Epoch 33: val_accuracy improved from 0.84533 to 0.85109, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 70s 3s/step - loss: 0.5450 - accuracy: 0.8325 - val_loss: 0.4912 - val_accuracy: 0.8511\n",
            "Epoch 34/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.8349\n",
            "Epoch 34: val_accuracy did not improve from 0.85109\n",
            "25/25 [==============================] - 70s 3s/step - loss: 0.5384 - accuracy: 0.8349 - val_loss: 0.5239 - val_accuracy: 0.8294\n",
            "Epoch 35/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.8364\n",
            "Epoch 35: val_accuracy did not improve from 0.85109\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.5332 - accuracy: 0.8364 - val_loss: 0.4903 - val_accuracy: 0.8434\n",
            "Epoch 36/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.8380\n",
            "Epoch 36: val_accuracy improved from 0.85109 to 0.85356, saving model to ./my_RNN_checkpoint/weights.hdf5\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.5247 - accuracy: 0.8380 - val_loss: 0.4722 - val_accuracy: 0.8536\n",
            "Epoch 37/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.8397\n",
            "Epoch 37: val_accuracy did not improve from 0.85356\n",
            "25/25 [==============================] - 69s 3s/step - loss: 0.5188 - accuracy: 0.8397 - val_loss: 0.4664 - val_accuracy: 0.8532\n",
            "Epoch 38/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.8404\n",
            "Epoch 38: val_accuracy did not improve from 0.85356\n",
            "25/25 [==============================] - 71s 3s/step - loss: 0.5151 - accuracy: 0.8404 - val_loss: 0.4843 - val_accuracy: 0.8456\n",
            "Epoch 39/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5096 - accuracy: 0.8431\n",
            "Epoch 39: val_accuracy did not improve from 0.85356\n",
            "25/25 [==============================] - 70s 3s/step - loss: 0.5096 - accuracy: 0.8431 - val_loss: 0.4722 - val_accuracy: 0.8485\n",
            "Epoch 40/40\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8449\n",
            "Epoch 40: val_accuracy did not improve from 0.85356\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.5031 - accuracy: 0.8449 - val_loss: 0.4631 - val_accuracy: 0.8511\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 0.4606 - accuracy: 0.8501\n",
            "Test Accuracy: 0.85006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hnQBqjCG9o4"
      },
      "source": [
        "### Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "RcmWsIAQgeAP",
        "outputId": "f7915a93-0831-455d-ca0e-d856004019f5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQVAtEuHFMB"
      },
      "source": [
        "### Helping Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_dict(dictionary):\n",
        "    reversed_dict = {}\n",
        "    for key, value in dictionary.items():\n",
        "        reversed_dict[value] = key\n",
        "    return reversed_dict\n",
        "\n",
        "reversed_dictionary = reverse_dict(encoded_classes)"
      ],
      "metadata": {
        "id": "HSOU5SKSggP0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pads(zipped, mask = 0):\n",
        "    out_true = []\n",
        "    out_pred = []\n",
        "    for zip_i in zipped:\n",
        "        a, b = tuple(zip_i)\n",
        "        if a != mask: # Exclude -PAD- term\n",
        "          out_true.append(reversed_dictionary[a])\n",
        "          if b not in reversed_dictionary: # The model classified something as 0 which is the pad class\n",
        "            random.seed(213)\n",
        "            random_number = random.randint(1, 18)\n",
        "            b = b + random_number # classify it as something else\n",
        "            if a==b:\n",
        "              if a<10: # if randomly we get b=a change b's value because we will get a correct classification but we don't want that\n",
        "                b = b + 1\n",
        "              else:\n",
        "                b = b - 1\n",
        "          out_pred.append(reversed_dictionary[b])\n",
        "\n",
        "    return out_true, out_pred\n",
        "\n",
        "def my_classification_report(x, y, model):\n",
        "\n",
        "  y_pred = model.predict(x, verbose=1).argmax(-1)\n",
        "  y_pred_flat = [item for sublist in y_pred for item in sublist] # flatten y_pred\n",
        "  y_true_flat = [item for sublist in y for item in sublist] # flatten y_true\n",
        "\n",
        "  tuples = zip(y_true_flat, y_pred_flat)\n",
        "  y_true, y_pred = remove_pads(tuples, 0)\n",
        "  return y_true, y_pred"
      ],
      "metadata": {
        "id": "0xEBTqddghFw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Sm0T7mHKte"
      },
      "source": [
        "### Model Assessment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  inputs = keras.Input((x_train_padded.shape[1],),)\n",
        "  mask = keras.layers.Masking(mask_value=0)(inputs) # Mask the pads\n",
        "  embeddings_layer = keras.layers.Embedding(input_dim = vocab_size, output_dim = embeddings.vectors.shape[1],  input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                          weights = [embedding_matrix], mask_zero = True, trainable = True)(mask)\n",
        "  x = keras.layers.BatchNormalization()(embeddings_layer) # Normalize\n",
        "  x = keras.layers.Bidirectional(keras.layers.LSTM(32, recurrent_dropout=0.4, return_sequences=True))(x) # Layer 1\n",
        "  #x = keras.layers.Bidirectional(keras.layers.LSTM(32, recurrent_dropout=0.5, return_sequences=True))(x) # Layer 2\n",
        "  x = keras.layers.Dropout(0.4)(x)\n",
        "  outputs = keras.layers.Dense(y_train_cat_padded.shape[2], activation = 'softmax')(x)\n",
        "  rnn_model = keras.Model(inputs, outputs)\n",
        "  \n",
        "  # Load weights from the pre-trained model\n",
        "  rnn_model.load_weights(\"./my_RNN_checkpoint/weights.hdf5\")\n",
        "  rnn_model.compile(\n",
        "      loss='categorical_crossentropy',\n",
        "      optimizer= SGD(learning_rate=0.1),\n",
        "      metrics=[\"accuracy\"]\n",
        "      )\n",
        "\n",
        "y_train_true, y_train_pred_RNN = my_classification_report(x_train_padded, y_train_padded, rnn_model)\n",
        "m_train_f1_score = f1_score(y_train_true, y_train_pred_RNN, average = 'macro')\n",
        "print(\"Train f1-score: {:.2f}% \\n\".format(m_train_f1_score*100))\n",
        "\n",
        "y_dev_true, y_dev_pred_RNN = my_classification_report(x_dev_padded, y_dev_padded, rnn_model)\n",
        "m_dev_f1_score = f1_score(y_dev_true, y_dev_pred_RNN, average = 'macro')\n",
        "print(\"Evaluation f1-score: {:.2f}% \\n\".format(m_dev_f1_score*100))\n",
        "\n",
        "y_test_true, y_test_pred_RNN = my_classification_report(x_test_padded, y_test_padded, rnn_model)\n",
        "m_test_f1_score = f1_score(y_test_true, y_test_pred_RNN, average = 'macro')\n",
        "print(\"Test f1-score: {:.2f}% \\n\".format(m_test_f1_score*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGMFl9JlgjWZ",
        "outputId": "cc5085f4-d5d0-494d-8c1d-adf68ebefdfa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 22s 223ms/step\n",
            "Train f1-score: 69.55% \n",
            "\n",
            "33/33 [==============================] - 7s 189ms/step\n",
            "Evaluation f1-score: 68.18% \n",
            "\n",
            "32/32 [==============================] - 7s 225ms/step\n",
            "Test f1-score: 68.06% \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk3vuFoZHQ4f"
      },
      "source": [
        "### Precision, Recall, AUC, ROC-AUC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BE CAREFUL! This function is used only for the next chunk of code. It is used to scale the dictionary from (1-18) to (0-17) in order to avoid index error. Do not use for any other code chunk!\n",
        "def encode_1d_list(list_1d, encoding_dict):\n",
        "    encoded_list_1d = []\n",
        "    for item in list_1d:\n",
        "        encoded_list_1d.append(encoding_dict[item])\n",
        "    for i in range(len(encoded_list_1d)):\n",
        "        encoded_list_1d[i] -= 1\n",
        "    return encoded_list_1d"
      ],
      "metadata": {
        "id": "XKMD4bliglWY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def errorCheck(y_true, y_pred):\n",
        "\n",
        "  # Find the unique classes in y_true and y_pred\n",
        "  unique_classes = np.unique(np.concatenate((y_true, y_pred)))\n",
        "\n",
        "  # Find the classes present in y_true but not in y_pred\n",
        "  missing_classes_y_pred = unique_classes[np.in1d(unique_classes, y_true) & ~np.in1d(unique_classes, y_pred)]\n",
        "\n",
        "  # Find the classes present in y_pred but not in y_true\n",
        "  missing_classes_y_true = unique_classes[np.in1d(unique_classes, y_pred) & ~np.in1d(unique_classes, y_true)]\n",
        "\n",
        "  # Calculate the sample size before modifying y_pred and y_true and my_max to know which has the extra class\n",
        "  if len(y_pred) > len(y_true):\n",
        "    my_max = y_pred\n",
        "  else:\n",
        "    my_max = y_true\n",
        "\n",
        "  sample_size = max(len(y_pred), len(y_true))\n",
        "\n",
        "  # Modify y_pred to include all the classes present in y_true\n",
        "  for c in missing_classes_y_pred:\n",
        "      y_pred = np.append(y_pred, c)\n",
        "\n",
        "  # Modify y_true to include all the classes present in y_pred\n",
        "  for c in missing_classes_y_true:\n",
        "      y_true = np.append(y_true, c)\n",
        "  \n",
        "  y_pred_onehot = np.eye(len(unique_classes))[y_pred[:sample_size]]\n",
        "  y_true_onehot = np.eye(len(unique_classes))[y_true[:sample_size]]\n",
        "\n",
        "  return y_true, y_true_onehot, y_pred, y_pred_onehot, my_max"
      ],
      "metadata": {
        "id": "jeilWmHTgmKf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_RNN_Results(x, y_true, y_pred):\n",
        "\n",
        "  y_pred_enc = encode_1d_list(y_pred, encoded_classes)\n",
        "  y_true_enc = encode_1d_list(y_true, encoded_classes)\n",
        "\n",
        "  train_precision_score = precision_score(y_true_enc, y_pred_enc, average = None)\n",
        "  train_recall_score = recall_score(y_true_enc, y_pred_enc, average = None)\n",
        "  train_f1_score = f1_score(y_true_enc, y_pred_enc, average = None)\n",
        "\n",
        "\n",
        "  m_train_precision_score = precision_score(y_true_enc, y_pred_enc, average = 'macro')\n",
        "  m_train_recall_score = recall_score(y_true_enc, y_pred_enc, average = 'macro')\n",
        "  m_train_f1_score = f1_score(y_true_enc, y_pred_enc, average = 'macro')\n",
        "\n",
        "  # Error checking\n",
        "\n",
        "  unique_pred = np.unique(y_pred_enc)\n",
        "  unique_true = np.unique(y_true_enc)\n",
        "\n",
        "  if unique_pred.shape != unique_true.shape:\n",
        "      y_true_upd, y_true_onehot, y_pred_upd, y_pred_onehot, my_max = errorCheck(y_true_enc, y_pred_enc)\n",
        "\n",
        "  elif unique_pred.shape == unique_true.shape and (unique_pred != unique_true).any():\n",
        "      y_true_upd, y_true_onehot, y_pred_upd, y_pred_onehot, my_max = errorCheck(y_true_enc, y_pred_enc)\n",
        "\n",
        "  else:\n",
        "    my_max = y_pred_enc # y_pred or y_true is the same thing in this case\n",
        "    # One-hot encode the predicted class labels\n",
        "    y_pred_onehot = np.eye(len(np.unique(y_pred_enc)))[y_pred_enc]\n",
        "\n",
        "    # One-hot encode the true class labels\n",
        "    y_true_onehot = np.eye(len(np.unique(y_true_enc)))[y_true_enc]\n",
        "\n",
        "\n",
        "  m_train_roc_auc_score = roc_auc_score(y_true_onehot, y_pred_onehot, multi_class= 'ovr', average = 'macro')\n",
        "\n",
        "  # auc calculation is a bit more messy\n",
        "\n",
        "  classes = np.unique(my_max) # get the classes of the model\n",
        "  train_roc_auc_score = {}\n",
        "  for i in range(len(classes)):\n",
        "      c = classes[i]\n",
        "      # Prepares an auxiliar dataframe to help with the plots\n",
        "      df_x = pd.DataFrame (x) # convert list to a dataframe\n",
        "      y_proba = y_pred_onehot # calculate the probabilities\n",
        "      df_aux = df_x.copy()\n",
        "      df_aux_flat = df_aux.values.flatten()\n",
        "\n",
        "      list_aux_flat = []\n",
        "      # Delete the paddings from the dataframe and \"normalize\" the value to be in the scale 0 - 17\n",
        "      for j in df_aux_flat:\n",
        "        if j!=0:\n",
        "          list_aux_flat.append(j)\n",
        "      df_aux_upd = pd.DataFrame (list_aux_flat)\n",
        "\n",
        "      df_aux_upd['class'] = [1 if y == c else 0 for y in y_true_enc]\n",
        "      df_aux_upd['prob'] = y_proba[:, i]\n",
        "      df_aux_upd = df_aux_upd.reset_index(drop = True)\n",
        "    \n",
        "      # Calculates the ROC AUC OvR\n",
        "      train_roc_auc_score[c] = roc_auc_score(df_aux_upd['class'], df_aux_upd['prob'])\n",
        "\n",
        "  table = [['    ', 'Precision', 'Recall', 'F1', 'AUC', 'Macro-Precision', 'Macro-Recall', 'Macro-F1', 'Macro-AUC'], \n",
        "           ['ADJ', round(train_precision_score[0], 3), round(train_recall_score[0], 3), round(train_f1_score[0], 3), round(train_roc_auc_score[0], 3), '-', '-', '-', '-'],\n",
        "           ['ADP', round(train_precision_score[1], 3), round(train_recall_score[1], 3), round(train_f1_score[1], 3), round(train_roc_auc_score[1], 3), '-', '-', '-', '-'],\n",
        "           ['ADV', round(train_precision_score[2], 3), round(train_recall_score[2], 3), round(train_f1_score[2], 3), round(train_roc_auc_score[2], 3), '-', '-', '-', '-'],\n",
        "           ['AUX', round(train_precision_score[3], 3), round(train_recall_score[3], 3), round(train_f1_score[3], 3), round(train_roc_auc_score[3], 3), '-', '-', '-', '-'],\n",
        "           ['CCONJ', round(train_precision_score[4], 3), round(train_recall_score[4], 3), round(train_f1_score[4], 3), round(train_roc_auc_score[4], 3), '-', '-', '-', '-'],\n",
        "           ['DET', round(train_precision_score[5], 3), round(train_recall_score[5], 3), round(train_f1_score[5], 3), round(train_roc_auc_score[5], 3), '-', '-', '-', '-'],\n",
        "           ['INTJ', round(train_precision_score[6], 3), round(train_recall_score[6], 3), round(train_f1_score[6], 3), round(train_roc_auc_score[6], 3), '-', '-', '-', '-'],\n",
        "           ['NOUN', round(train_precision_score[7], 3), round(train_recall_score[7], 3), round(train_f1_score[7], 3), round(train_roc_auc_score[7], 3), '-', '-', '-', '-'],\n",
        "           ['NUM', round(train_precision_score[8], 3), round(train_recall_score[8], 3), round(train_f1_score[8], 3), round(train_roc_auc_score[8], 3), '-', '-', '-', '-'],\n",
        "           ['PART', round(train_precision_score[9], 3), round(train_recall_score[9], 3), round(train_f1_score[9], 3), round(train_roc_auc_score[9], 3), '-', '-', '-', '-'],\n",
        "           ['PRON', round(train_precision_score[10], 3), round(train_recall_score[10], 3), round(train_f1_score[10], 3), round(train_roc_auc_score[10], 3), '-', '-', '-', '-'],\n",
        "           ['PROPN', round(train_precision_score[11], 3), round(train_recall_score[11], 3), round(train_f1_score[11], 3), round(train_roc_auc_score[11], 3), '-', '-', '-', '-'],\n",
        "           ['PUNCT', round(train_precision_score[12], 3), round(train_recall_score[12], 3), round(train_f1_score[12], 3), round(train_roc_auc_score[12], 3), '-', '-', '-', '-'],\n",
        "           ['SCONJ', round(train_precision_score[13], 3), round(train_recall_score[13], 3), round(train_f1_score[13], 3), round(train_roc_auc_score[13], 3), '-', '-', '-', '-'],\n",
        "           ['SPACE', round(train_precision_score[14], 3), round(train_recall_score[14], 3), round(train_f1_score[14], 3), round(train_roc_auc_score[14], 3), '-', '-', '-', '-'],\n",
        "           ['SYM', round(train_precision_score[15], 3), round(train_recall_score[15], 3), round(train_f1_score[15], 3), round(train_roc_auc_score[15], 3), '-', '-', '-', '-'],\n",
        "           ['VERB', round(train_precision_score[16], 3), round(train_recall_score[16], 3), round(train_f1_score[16], 3), round(train_roc_auc_score[16], 3), '-', '-', '-', '-'],\n",
        "           ['X', round(train_precision_score[17], 3), round(train_recall_score[17], 3), round(train_f1_score[17], 3), round(train_roc_auc_score[17], 3), '-', '-', '-', '-'],\n",
        "           ['Total', '-', '-','-','-', round(m_train_precision_score, 3), round(m_train_recall_score, 3), round(m_train_f1_score, 3), round(m_train_roc_auc_score, 3)]]\n",
        "\n",
        "  print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
        "  return table"
      ],
      "metadata": {
        "id": "8JEbSufugnnh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN ---------------------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "print(\"The training results: \\n\")\n",
        "rnn_train_table = calculate_RNN_Results(x_train_padded, y_train_true, y_train_pred_RNN)\n",
        "\n",
        "print(\"The evaluation results: \\n\")\n",
        "rnn_dev_table = calculate_RNN_Results(x_dev_padded, y_dev_true, y_dev_pred_RNN)\n",
        "\n",
        "print(\"The test results: \\n\")\n",
        "rnn_test_table = calculate_RNN_Results(x_test_padded, y_test_true, y_test_pred_RNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xe7-rt_govM",
        "outputId": "6fee916d-a59c-4f4a-fd28-e9e38b65cb5c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN ---------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The training results: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â•’â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚        â”‚ Precision   â”‚ Recall   â”‚ F1    â”‚ AUC   â”‚ Macro-Precision   â”‚ Macro-Recall   â”‚ Macro-F1   â”‚ Macro-AUC   â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ ADJ    â”‚ 0.788       â”‚ 0.45     â”‚ 0.573 â”‚ 0.721 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADP    â”‚ 0.924       â”‚ 0.948    â”‚ 0.936 â”‚ 0.97  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADV    â”‚ 0.875       â”‚ 0.613    â”‚ 0.721 â”‚ 0.805 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ AUX    â”‚ 0.958       â”‚ 0.881    â”‚ 0.918 â”‚ 0.94  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ CCONJ  â”‚ 0.997       â”‚ 0.978    â”‚ 0.988 â”‚ 0.989 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ DET    â”‚ 0.957       â”‚ 0.968    â”‚ 0.963 â”‚ 0.982 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ INTJ   â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NOUN   â”‚ 0.711       â”‚ 0.931    â”‚ 0.806 â”‚ 0.929 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NUM    â”‚ 0.869       â”‚ 0.485    â”‚ 0.623 â”‚ 0.742 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PART   â”‚ 0.886       â”‚ 0.983    â”‚ 0.932 â”‚ 0.99  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PRON   â”‚ 0.939       â”‚ 0.954    â”‚ 0.946 â”‚ 0.974 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PROPN  â”‚ 0.852       â”‚ 0.504    â”‚ 0.634 â”‚ 0.75  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PUNCT  â”‚ 0.995       â”‚ 0.999    â”‚ 0.997 â”‚ 0.999 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SCONJ  â”‚ 0.962       â”‚ 0.547    â”‚ 0.698 â”‚ 0.774 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SPACE  â”‚ 1.0         â”‚ 0.998    â”‚ 0.999 â”‚ 0.999 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SYM    â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ VERB   â”‚ 0.747       â”‚ 0.83     â”‚ 0.786 â”‚ 0.898 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ X      â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Total  â”‚ -           â”‚ -        â”‚ -     â”‚ -     â”‚ 0.748             â”‚ 0.671          â”‚ 0.695      â”‚ 0.831       â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "The evaluation results: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â•’â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚        â”‚ Precision   â”‚ Recall   â”‚ F1    â”‚ AUC   â”‚ Macro-Precision   â”‚ Macro-Recall   â”‚ Macro-F1   â”‚ Macro-AUC   â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ ADJ    â”‚ 0.792       â”‚ 0.412    â”‚ 0.542 â”‚ 0.703 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADP    â”‚ 0.917       â”‚ 0.95     â”‚ 0.933 â”‚ 0.97  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADV    â”‚ 0.881       â”‚ 0.55     â”‚ 0.677 â”‚ 0.773 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ AUX    â”‚ 0.952       â”‚ 0.891    â”‚ 0.92  â”‚ 0.944 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ CCONJ  â”‚ 0.997       â”‚ 0.99     â”‚ 0.993 â”‚ 0.995 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ DET    â”‚ 0.96        â”‚ 0.971    â”‚ 0.966 â”‚ 0.984 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ INTJ   â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NOUN   â”‚ 0.678       â”‚ 0.905    â”‚ 0.775 â”‚ 0.911 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NUM    â”‚ 0.848       â”‚ 0.482    â”‚ 0.615 â”‚ 0.741 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PART   â”‚ 0.892       â”‚ 0.972    â”‚ 0.93  â”‚ 0.984 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PRON   â”‚ 0.939       â”‚ 0.957    â”‚ 0.948 â”‚ 0.975 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PROPN  â”‚ 0.771       â”‚ 0.419    â”‚ 0.543 â”‚ 0.706 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PUNCT  â”‚ 0.992       â”‚ 0.998    â”‚ 0.995 â”‚ 0.999 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SCONJ  â”‚ 0.953       â”‚ 0.531    â”‚ 0.682 â”‚ 0.765 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SPACE  â”‚ 1.0         â”‚ 0.994    â”‚ 0.997 â”‚ 0.997 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SYM    â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ VERB   â”‚ 0.703       â”‚ 0.816    â”‚ 0.756 â”‚ 0.888 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ X      â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Total  â”‚ -           â”‚ -        â”‚ -     â”‚ -     â”‚ 0.737             â”‚ 0.658          â”‚ 0.682      â”‚ 0.824       â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "The test results: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â•’â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚        â”‚ Precision   â”‚ Recall   â”‚ F1    â”‚ AUC   â”‚ Macro-Precision   â”‚ Macro-Recall   â”‚ Macro-F1   â”‚ Macro-AUC   â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ ADJ    â”‚ 0.794       â”‚ 0.399    â”‚ 0.531 â”‚ 0.696 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADP    â”‚ 0.94        â”‚ 0.954    â”‚ 0.947 â”‚ 0.974 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ ADV    â”‚ 0.873       â”‚ 0.57     â”‚ 0.69  â”‚ 0.783 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ AUX    â”‚ 0.952       â”‚ 0.896    â”‚ 0.923 â”‚ 0.947 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ CCONJ  â”‚ 1.0         â”‚ 0.98     â”‚ 0.99  â”‚ 0.99  â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ DET    â”‚ 0.957       â”‚ 0.978    â”‚ 0.968 â”‚ 0.987 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ INTJ   â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NOUN   â”‚ 0.673       â”‚ 0.912    â”‚ 0.775 â”‚ 0.914 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ NUM    â”‚ 0.8         â”‚ 0.444    â”‚ 0.571 â”‚ 0.722 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PART   â”‚ 0.906       â”‚ 0.986    â”‚ 0.944 â”‚ 0.992 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PRON   â”‚ 0.951       â”‚ 0.956    â”‚ 0.953 â”‚ 0.975 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PROPN  â”‚ 0.695       â”‚ 0.365    â”‚ 0.479 â”‚ 0.679 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ PUNCT  â”‚ 0.994       â”‚ 0.999    â”‚ 0.996 â”‚ 0.999 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SCONJ  â”‚ 0.96        â”‚ 0.59     â”‚ 0.731 â”‚ 0.795 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SPACE  â”‚ 1.0         â”‚ 0.994    â”‚ 0.997 â”‚ 0.997 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ SYM    â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ VERB   â”‚ 0.704       â”‚ 0.818    â”‚ 0.757 â”‚ 0.888 â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ X      â”‚ 0.0         â”‚ 0.0      â”‚ 0.0   â”‚ 0.5   â”‚ -                 â”‚ -              â”‚ -          â”‚ -           â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Total  â”‚ -           â”‚ -        â”‚ -     â”‚ -     â”‚ 0.733             â”‚ 0.658          â”‚ 0.681      â”‚ 0.824       â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16gbQBOfHxDH"
      },
      "source": [
        "### Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  \n",
        "  def nn_model(neurons = 64, optimizer = 'Adam', recurrent_dropout = 0.5, dropout = 0.5, trainable = True, batch_size = 512, learning_rate = 0.01):\n",
        "    inputs = keras.Input((x_train_padded.shape[1],),)\n",
        "    mask = keras.layers.Masking(mask_value=0)(inputs) # Mask the pads\n",
        "    embeddings_layer = keras.layers.Embedding(input_dim = vocab_size, output_dim = embeddings.vectors.shape[1],  input_length = MAX_SEQUENCE_LENGTH,\n",
        "                                          weights = [embedding_matrix], mask_zero = True, trainable = trainable)(mask)\n",
        "    x = keras.layers.BatchNormalization()(embeddings_layer) # Normalize\n",
        "    x = keras.layers.Bidirectional(keras.layers.LSTM(neurons, recurrent_dropout= recurrent_dropout, return_sequences=True))(x) # Layer 1\n",
        "    x = keras.layers.Dropout(dropout)(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(y_dev_cat_padded.shape[2])(x)\n",
        "    outputs = keras.layers.Dense(y_dev_cat_padded.shape[2], activation='softmax')(x)\n",
        "    rnn_model = keras.Model(inputs, outputs)\n",
        "    rnn_model.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=learning_rate), metrics=['categorical_accuracy'])\n",
        "    print(rnn_model.summary())\n",
        "\n",
        "    return rnn_model\n",
        "\n",
        "  def tune_lr(optimizer, learning_rate):\n",
        "    optimizerDict = {'Adam': Adam(learning_rate=learning_rate), 'SGD': SGD(learning_rate=learning_rate), 'RMSprop': RMSprop(learning_rate=learning_rate)}\n",
        "    result = optimizerDict[optimizer]\n",
        "    print(result)\n",
        "    return result\n",
        "\n",
        "  optimizer = ['SGD', 'Adam', 'RMSprop']\n",
        "  trainable = [True, False]\n",
        "  dropout = [0.2, 0.4, 0.5]\n",
        "  recurrent_dropout = [0.2, 0.4, 0.5]\n",
        "  neurons = [16, 32, 64]\n",
        "  batch_size = [64, 128, 256, 512]\n",
        "  learning_rate = [0.1, 0.01, 0.001]\n",
        "  param_grid = dict(neurons = neurons, optimizer = optimizer, recurrent_dropout = recurrent_dropout, dropout = dropout, trainable = trainable,\n",
        "                    batch_size = batch_size, learning_rate = learning_rate)\n",
        "  clf = KerasClassifier(build_fn = nn_model, epochs = 5, verbose = 2, neurons = neurons, optimizer = optimizer, recurrent_dropout = recurrent_dropout, dropout = dropout,\n",
        "                        trainable = trainable, batch_size = batch_size, learning_rate = learning_rate)\n",
        "  rnn_model = RandomizedSearchCV(estimator= clf, cv = 5, param_distributions = param_grid, n_jobs=-1,verbose = 2,random_state=1234)\n",
        "  nsamples, nx, ny = y_dev_cat_padded.shape\n",
        "  y_dev_cat_padded_2d = y_dev_cat_padded[:, 0, :]  # 1045, 300, 19 --> 1045, 19\n",
        "  #y_dev_cat_padded_2d = y_dev_cat_padded.reshape(nsamples, nx* ny) # 1045, 300, 19 --> 1045, 300*19 --> 1045, 5700\n",
        "  rnn_model.fit(x_dev_padded, y_dev_cat_padded_2d)\n",
        "\n",
        "print(\"Best estimator \\n\", rnn_model.best_estimator_)\n",
        "print(\"Best score \\n\", rnn_model.best_score_)\n",
        "print(\"Best params \\n\", rnn_model.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BputtF2gr6j",
        "outputId": "e02a28e4-022e-4553-ea67-04446cf32879"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/scikeras/wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 300)]             0         \n",
            "                                                                 \n",
            " masking_2 (Masking)         (None, 300)               0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 300, 300)          1874700   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 300, 300)         1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 300, 128)         186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 300, 128)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 38400)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 19)                729619    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 19)                380       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,792,779\n",
            "Trainable params: 917,479\n",
            "Non-trainable params: 1,875,300\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "3/3 - 15s - loss: 2.7712 - categorical_accuracy: 0.2048 - 15s/epoch - 5s/step\n",
            "Epoch 2/5\n",
            "3/3 - 8s - loss: 2.4099 - categorical_accuracy: 0.3656 - 8s/epoch - 3s/step\n",
            "Epoch 3/5\n",
            "3/3 - 9s - loss: 2.2005 - categorical_accuracy: 0.3885 - 9s/epoch - 3s/step\n",
            "Epoch 4/5\n",
            "3/3 - 9s - loss: 2.0856 - categorical_accuracy: 0.3770 - 9s/epoch - 3s/step\n",
            "Epoch 5/5\n",
            "3/3 - 7s - loss: 1.9504 - categorical_accuracy: 0.4794 - 7s/epoch - 2s/step\n",
            "Best estimator \n",
            " KerasClassifier(\n",
            "\tmodel=None\n",
            "\tbuild_fn=<function nn_model at 0x7f4af8b2a9d0>\n",
            "\twarm_start=False\n",
            "\trandom_state=None\n",
            "\toptimizer=SGD\n",
            "\tloss=None\n",
            "\tmetrics=None\n",
            "\tbatch_size=512\n",
            "\tvalidation_batch_size=None\n",
            "\tverbose=2\n",
            "\tcallbacks=None\n",
            "\tvalidation_split=0.0\n",
            "\tshuffle=True\n",
            "\trun_eagerly=False\n",
            "\tepochs=5\n",
            "\tneurons=64\n",
            "\trecurrent_dropout=0.5\n",
            "\tkernel_constraint=2.0\n",
            "\tdropout=0.2\n",
            "\ttrainable=False\n",
            "\tlearning_rate=0.001\n",
            "\tclass_weight=None\n",
            ")\n",
            "Best score \n",
            " 0.3645933014354067\n",
            "Best params \n",
            " {'trainable': False, 'recurrent_dropout': 0.5, 'optimizer': 'SGD', 'neurons': 64, 'learning_rate': 0.001, 'kernel_constraint': 2.0, 'dropout': 0.2, 'batch_size': 512}\n"
          ]
        }
      ]
    }
  ]
}