# Natural Language Processing (NLP)

Natural Language Processing is a Master's course taught at AUEB. During my final year of studies after consulting the teacher of the course (and later my advisor of my bachelor thesis),
we decided since I was intrigued by the topic, I should take the course unofficially. These are the group projects (normally done by 3-4 students) that are required by the
course. I had to do them individually since I wasn't officially a student of the program. The students had to choose either Sentiment
Analysis or Part of Speech Tagging and implement their choice with:

1. Logistic Regression, KNN  (*only for those who chose Sentiment Analysis*)
2. MLP
3. RNN
4. CNN
5. Pretrained BERT

Eventually I ended up doing both Sentiment Analysis and POS tagging, so I have added both projects to this repository. Lastly, I should note that it is possible that the performance
of the models can be improved easily in numerous cases, but I did not have the time for additional fine tuning. After all my main goal was to refresh
some of these methods and learn others from scratch. 
I encourage you to grab the code and perfect those models! 

## Course Syllabus

The syllabus of the course included:

- N-gram language models, entropy, cross-entropy, perplexity, context-aware spelling correction, beam-search decoding.
- Boolean and TF-IDF features.
- Information gain, Singular Value Decomposition (SVD).
- k-Nearest Neighbors (k-NN).
- Naive Bayes.
- Evaluation Metrics: Precision, recall, F1, Area Under the Curve (AUC).
- k-means.
- Linear and logistic regression, stochastic gradient descent.
- Perceptrons, Multi-Layer Perceptrons (MLPs), backpropagation.
- Dropout, batch/layer normalization.
- Pre-training word embeddings, Word2Vec.
- Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs)/Long Short-Term Memory (LSTM), RNN language models, RNNs with self-attention, bidirectional, stacked, hierarchical RNNs, encoder-decoder RNNs.
- Text processing with Convolutional Neural Networks (CNNs).
- Transformer encoders and decoders.
- Pre-trained Transformers, including BERT, BART, T5, GPT-x, InstructGPT, ChatGPT, fine-tuning, prompting.
- Data augmentation.
- Applications in spelling correction, sentiment analysis, information extraction, machine translation, image captioning, question answering, dialogue systems.
